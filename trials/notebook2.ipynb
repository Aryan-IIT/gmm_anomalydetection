{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae04ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### code to generate toy dataset in the same folder. \n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "IMG_SIZE = 64\n",
    "NORMAL_SHAPES = ['circle', 'square']\n",
    "NORMAL_COLORS = ['red', 'green', 'blue']\n",
    "NUM_NORMAL = 200\n",
    "NUM_ANOMALY = 20\n",
    "OUTPUT_DIR = \"dataset\"\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Utility: Create folders\n",
    "def make_dirs():\n",
    "    for split in ['train', 'test']:\n",
    "        for cls in ['normal', 'anomalous']:\n",
    "            os.makedirs(os.path.join(OUTPUT_DIR, split, cls), exist_ok=True)\n",
    "\n",
    "# Generate a normal image\n",
    "def generate_normal_image():\n",
    "    img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), 'black')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    shape = random.choice(NORMAL_SHAPES)\n",
    "    color = random.choice(NORMAL_COLORS)\n",
    "    x0, y0, x1, y1 = 16, 16, 48, 48\n",
    "\n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([x0, y0, x1, y1], fill=color)\n",
    "    elif shape == 'square':\n",
    "        draw.rectangle([x0, y0, x1, y1], fill=color)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Generate an anomalous image (plain white)\n",
    "def generate_anomalous_image():\n",
    "    return Image.new('RGB', (IMG_SIZE, IMG_SIZE), 'white')\n",
    "\n",
    "# Save image\n",
    "def save_image(img, split, cls, idx):\n",
    "    path = os.path.join(OUTPUT_DIR, split, cls, f\"{cls}_{idx:03d}.png\")\n",
    "    img.save(path)\n",
    "\n",
    "# Main generation logic\n",
    "def generate_dataset():\n",
    "    make_dirs()\n",
    "\n",
    "    # TRAIN: 100 normal\n",
    "    print(\"Generating training set...\")\n",
    "    for i in tqdm(range(100)):\n",
    "        img = generate_normal_image()\n",
    "        save_image(img, 'train', 'normal', i)\n",
    "\n",
    "    # TEST: 100 normal + 20 anomalous\n",
    "    print(\"Generating test set...\")\n",
    "    for i in tqdm(range(100)):\n",
    "        img = generate_normal_image()\n",
    "        save_image(img, 'test', 'normal', i)\n",
    "    for i in tqdm(range(20)):\n",
    "        img = generate_anomalous_image()\n",
    "        save_image(img, 'test', 'anomalous', i)\n",
    "\n",
    "generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d956fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompressionNetwork from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/compression_network.py\n",
      "DAGMM   from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/model.py\n"
     ]
    }
   ],
   "source": [
    "# 0) Point to your CNN‑DAGMM folder *before* any imports\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "cnn_dir = Path.cwd().parent / \"cnn_dagmm\"\n",
    "sys.path.insert(0, str(cnn_dir))\n",
    "\n",
    "# 1) Import & force‑reload to clear any old cache\n",
    "import compression_network, model\n",
    "importlib.reload(compression_network)\n",
    "importlib.reload(model)\n",
    "\n",
    "# 2) Verify you’re using the right files\n",
    "print(\"CompressionNetwork from:\", compression_network.__file__)\n",
    "print(\"DAGMM   from:\",           model.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e4ae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: {'normal': 100}\n",
      "Test  class counts: {'anomalous': 20, 'normal': 100}\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # DAGMM Anomaly Detection with Sampled Anomalies in Train\n",
    "\n",
    "# %% [code]\n",
    "# 1) Make your DAGMM code importable\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"cnn_dagmm\"))\n",
    "\n",
    "# %% [code]\n",
    "# 2) Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from collections import Counter\n",
    "\n",
    "from model import DAGMM  # your revised model.py\n",
    "\n",
    "# %% [code]\n",
    "# 3) Data transforms & loaders (images → flattened vectors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(\"dataset/train\", transform=transform, allow_empty=True)\n",
    "test_ds  = datasets.ImageFolder(\"dataset/test\",  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# Print ground‑truth counts\n",
    "print(\"Train class counts:\", {train_ds.classes[k]: v for k,v in Counter(train_ds.targets).items()})\n",
    "print(\"Test  class counts:\", {test_ds.classes[k]: v for k,v in Counter(test_ds.targets).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca88aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imgs, labels in train_loader:\n",
    "#     labels = labels.numpy()\n",
    "#     print(\"Test batch labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6accced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3286, grad_fn=<AddBackward0>) tensor(1820.6508, grad_fn=<MeanBackward0>) tensor(551.9995, grad_fn=<SumBackward0>)\n",
      "tensor(0.3042, grad_fn=<AddBackward0>) tensor(1752.5901, grad_fn=<MeanBackward0>) tensor(551.1207, grad_fn=<SumBackward0>)\n",
      "tensor(0.2763, grad_fn=<AddBackward0>) tensor(1555.4847, grad_fn=<MeanBackward0>) tensor(550.1384, grad_fn=<SumBackward0>)\n",
      "Epoch 1/50 — avg train loss: 847.3391\n",
      "tensor(0.2579, grad_fn=<AddBackward0>) tensor(1448.5103, grad_fn=<MeanBackward0>) tensor(549.1854, grad_fn=<SumBackward0>)\n",
      "tensor(0.2418, grad_fn=<AddBackward0>) tensor(1339.3032, grad_fn=<MeanBackward0>) tensor(548.2668, grad_fn=<SumBackward0>)\n",
      "tensor(0.2253, grad_fn=<AddBackward0>) tensor(1237.6212, grad_fn=<MeanBackward0>) tensor(547.3755, grad_fn=<SumBackward0>)\n",
      "Epoch 2/50 — avg train loss: 670.6188\n",
      "tensor(0.2169, grad_fn=<AddBackward0>) tensor(1197.9111, grad_fn=<MeanBackward0>) tensor(546.5061, grad_fn=<SumBackward0>)\n",
      "tensor(0.2142, grad_fn=<AddBackward0>) tensor(1148.2579, grad_fn=<MeanBackward0>) tensor(545.6512, grad_fn=<SumBackward0>)\n",
      "tensor(0.2027, grad_fn=<AddBackward0>) tensor(1112.6453, grad_fn=<MeanBackward0>) tensor(544.8067, grad_fn=<SumBackward0>)\n",
      "Epoch 3/50 — avg train loss: 579.8046\n",
      "tensor(0.1931, grad_fn=<AddBackward0>) tensor(1051.9636, grad_fn=<MeanBackward0>) tensor(543.9632, grad_fn=<SumBackward0>)\n",
      "tensor(0.1874, grad_fn=<AddBackward0>) tensor(1022.3940, grad_fn=<MeanBackward0>) tensor(543.1162, grad_fn=<SumBackward0>)\n",
      "tensor(0.1798, grad_fn=<AddBackward0>) tensor(958.9549, grad_fn=<MeanBackward0>) tensor(542.2642, grad_fn=<SumBackward0>)\n",
      "Epoch 4/50 — avg train loss: 511.5788\n",
      "tensor(0.1757, grad_fn=<AddBackward0>) tensor(924.9282, grad_fn=<MeanBackward0>) tensor(541.4063, grad_fn=<SumBackward0>)\n",
      "tensor(0.1643, grad_fn=<AddBackward0>) tensor(861.4547, grad_fn=<MeanBackward0>) tensor(540.5399, grad_fn=<SumBackward0>)\n",
      "tensor(0.1583, grad_fn=<AddBackward0>) tensor(831.0502, grad_fn=<MeanBackward0>) tensor(539.6606, grad_fn=<SumBackward0>)\n",
      "Epoch 5/50 — avg train loss: 444.8944\n",
      "tensor(0.1515, grad_fn=<AddBackward0>) tensor(790.7349, grad_fn=<MeanBackward0>) tensor(538.7647, grad_fn=<SumBackward0>)\n",
      "tensor(0.1455, grad_fn=<AddBackward0>) tensor(755.0660, grad_fn=<MeanBackward0>) tensor(537.8492, grad_fn=<SumBackward0>)\n",
      "tensor(0.1381, grad_fn=<AddBackward0>) tensor(716.3251, grad_fn=<MeanBackward0>) tensor(536.9122, grad_fn=<SumBackward0>)\n",
      "Epoch 6/50 — avg train loss: 387.8958\n",
      "tensor(0.1343, grad_fn=<AddBackward0>) tensor(688.6113, grad_fn=<MeanBackward0>) tensor(535.9521, grad_fn=<SumBackward0>)\n",
      "tensor(0.1267, grad_fn=<AddBackward0>) tensor(652.3419, grad_fn=<MeanBackward0>) tensor(534.9679, grad_fn=<SumBackward0>)\n",
      "tensor(0.1224, grad_fn=<AddBackward0>) tensor(637.9828, grad_fn=<MeanBackward0>) tensor(533.9579, grad_fn=<SumBackward0>)\n",
      "Epoch 7/50 — avg train loss: 342.4305\n",
      "tensor(0.1179, grad_fn=<AddBackward0>) tensor(614.5809, grad_fn=<MeanBackward0>) tensor(532.9194, grad_fn=<SumBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>) tensor(570.1965, grad_fn=<MeanBackward0>) tensor(531.8510, grad_fn=<SumBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>) tensor(539.7867, grad_fn=<MeanBackward0>) tensor(530.7529, grad_fn=<SumBackward0>)\n",
      "Epoch 8/50 — avg train loss: 301.5653\n",
      "tensor(0.1025, grad_fn=<AddBackward0>) tensor(526.9780, grad_fn=<MeanBackward0>) tensor(529.6255, grad_fn=<SumBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>) tensor(486.6927, grad_fn=<MeanBackward0>) tensor(528.4678, grad_fn=<SumBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>) tensor(472.1046, grad_fn=<MeanBackward0>) tensor(527.2788, grad_fn=<SumBackward0>)\n",
      "Epoch 9/50 — avg train loss: 263.1828\n",
      "tensor(0.0884, grad_fn=<AddBackward0>) tensor(453.3312, grad_fn=<MeanBackward0>) tensor(526.0571, grad_fn=<SumBackward0>)\n",
      "tensor(0.0831, grad_fn=<AddBackward0>) tensor(425.9091, grad_fn=<MeanBackward0>) tensor(524.8016, grad_fn=<SumBackward0>)\n",
      "tensor(0.0813, grad_fn=<AddBackward0>) tensor(418.2162, grad_fn=<MeanBackward0>) tensor(523.5112, grad_fn=<SumBackward0>)\n",
      "Epoch 10/50 — avg train loss: 232.8638\n",
      "tensor(0.0754, grad_fn=<AddBackward0>) tensor(387.5774, grad_fn=<MeanBackward0>) tensor(522.1859, grad_fn=<SumBackward0>)\n",
      "tensor(0.0791, grad_fn=<AddBackward0>) tensor(412.4213, grad_fn=<MeanBackward0>) tensor(520.8256, grad_fn=<SumBackward0>)\n",
      "tensor(0.0702, grad_fn=<AddBackward0>) tensor(360.2017, grad_fn=<MeanBackward0>) tensor(519.4297, grad_fn=<SumBackward0>)\n",
      "Epoch 11/50 — avg train loss: 210.7030\n",
      "tensor(0.0699, grad_fn=<AddBackward0>) tensor(363.4993, grad_fn=<MeanBackward0>) tensor(517.9983, grad_fn=<SumBackward0>)\n",
      "tensor(0.0662, grad_fn=<AddBackward0>) tensor(343.6047, grad_fn=<MeanBackward0>) tensor(516.5303, grad_fn=<SumBackward0>)\n",
      "tensor(0.0623, grad_fn=<AddBackward0>) tensor(324.6568, grad_fn=<MeanBackward0>) tensor(515.0258, grad_fn=<SumBackward0>)\n",
      "Epoch 12/50 — avg train loss: 189.9381\n",
      "tensor(0.0646, grad_fn=<AddBackward0>) tensor(337.5146, grad_fn=<MeanBackward0>) tensor(513.4844, grad_fn=<SumBackward0>)\n",
      "tensor(0.0581, grad_fn=<AddBackward0>) tensor(305.4728, grad_fn=<MeanBackward0>) tensor(511.9081, grad_fn=<SumBackward0>)\n",
      "tensor(0.0550, grad_fn=<AddBackward0>) tensor(290.0645, grad_fn=<MeanBackward0>) tensor(510.2946, grad_fn=<SumBackward0>)\n",
      "Epoch 13/50 — avg train loss: 173.9162\n",
      "tensor(0.0537, grad_fn=<AddBackward0>) tensor(283.0222, grad_fn=<MeanBackward0>) tensor(508.6385, grad_fn=<SumBackward0>)\n",
      "tensor(0.0546, grad_fn=<AddBackward0>) tensor(288.9781, grad_fn=<MeanBackward0>) tensor(506.9320, grad_fn=<SumBackward0>)\n",
      "tensor(0.0532, grad_fn=<AddBackward0>) tensor(285.0908, grad_fn=<MeanBackward0>) tensor(505.1667, grad_fn=<SumBackward0>)\n",
      "Epoch 14/50 — avg train loss: 161.5180\n",
      "tensor(0.0537, grad_fn=<AddBackward0>) tensor(287.3113, grad_fn=<MeanBackward0>) tensor(503.3344, grad_fn=<SumBackward0>)\n",
      "tensor(0.0478, grad_fn=<AddBackward0>) tensor(257.6933, grad_fn=<MeanBackward0>) tensor(501.4296, grad_fn=<SumBackward0>)\n",
      "tensor(0.0446, grad_fn=<AddBackward0>) tensor(244.0708, grad_fn=<MeanBackward0>) tensor(499.4458, grad_fn=<SumBackward0>)\n",
      "Epoch 15/50 — avg train loss: 150.3662\n",
      "tensor(0.0465, grad_fn=<AddBackward0>) tensor(254.7972, grad_fn=<MeanBackward0>) tensor(497.3770, grad_fn=<SumBackward0>)\n",
      "tensor(0.0441, grad_fn=<AddBackward0>) tensor(243.5094, grad_fn=<MeanBackward0>) tensor(495.2189, grad_fn=<SumBackward0>)\n",
      "tensor(0.0425, grad_fn=<AddBackward0>) tensor(237.7215, grad_fn=<MeanBackward0>) tensor(492.9673, grad_fn=<SumBackward0>)\n",
      "Epoch 16/50 — avg train loss: 141.5761\n",
      "tensor(0.0414, grad_fn=<AddBackward0>) tensor(231.5904, grad_fn=<MeanBackward0>) tensor(490.6190, grad_fn=<SumBackward0>)\n",
      "tensor(0.0407, grad_fn=<AddBackward0>) tensor(228.6430, grad_fn=<MeanBackward0>) tensor(488.1720, grad_fn=<SumBackward0>)\n",
      "tensor(0.0402, grad_fn=<AddBackward0>) tensor(224.9230, grad_fn=<MeanBackward0>) tensor(485.6254, grad_fn=<SumBackward0>)\n",
      "Epoch 17/50 — avg train loss: 133.0948\n",
      "tensor(0.0375, grad_fn=<AddBackward0>) tensor(211.9116, grad_fn=<MeanBackward0>) tensor(482.9789, grad_fn=<SumBackward0>)\n",
      "tensor(0.0376, grad_fn=<AddBackward0>) tensor(212.2297, grad_fn=<MeanBackward0>) tensor(480.2330, grad_fn=<SumBackward0>)\n",
      "tensor(0.0377, grad_fn=<AddBackward0>) tensor(212.9672, grad_fn=<MeanBackward0>) tensor(477.3892, grad_fn=<SumBackward0>)\n",
      "Epoch 18/50 — avg train loss: 125.0231\n",
      "tensor(0.0350, grad_fn=<AddBackward0>) tensor(199.2424, grad_fn=<MeanBackward0>) tensor(474.4505, grad_fn=<SumBackward0>)\n",
      "tensor(0.0344, grad_fn=<AddBackward0>) tensor(196.1652, grad_fn=<MeanBackward0>) tensor(471.4195, grad_fn=<SumBackward0>)\n",
      "tensor(0.0333, grad_fn=<AddBackward0>) tensor(190.5416, grad_fn=<MeanBackward0>) tensor(468.2996, grad_fn=<SumBackward0>)\n",
      "Epoch 19/50 — avg train loss: 116.4114\n",
      "tensor(0.0324, grad_fn=<AddBackward0>) tensor(187.1148, grad_fn=<MeanBackward0>) tensor(465.0939, grad_fn=<SumBackward0>)\n",
      "tensor(0.0335, grad_fn=<AddBackward0>) tensor(193.6455, grad_fn=<MeanBackward0>) tensor(461.8066, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(179.8295, grad_fn=<MeanBackward0>) tensor(458.4422, grad_fn=<SumBackward0>)\n",
      "Epoch 20/50 — avg train loss: 111.8906\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(176.2200, grad_fn=<MeanBackward0>) tensor(455.0042, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(174.8132, grad_fn=<MeanBackward0>) tensor(451.4999, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(171.3915, grad_fn=<MeanBackward0>) tensor(447.9362, grad_fn=<SumBackward0>)\n",
      "Epoch 21/50 — avg train loss: 105.2872\n",
      "tensor(0.0273, grad_fn=<AddBackward0>) tensor(164.3585, grad_fn=<MeanBackward0>) tensor(444.3196, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(167.3634, grad_fn=<MeanBackward0>) tensor(440.6565, grad_fn=<SumBackward0>)\n",
      "tensor(0.0264, grad_fn=<AddBackward0>) tensor(160.7827, grad_fn=<MeanBackward0>) tensor(436.9533, grad_fn=<SumBackward0>)\n",
      "Epoch 22/50 — avg train loss: 99.9777\n",
      "tensor(0.0256, grad_fn=<AddBackward0>) tensor(156.4037, grad_fn=<MeanBackward0>) tensor(433.2169, grad_fn=<SumBackward0>)\n",
      "tensor(0.0254, grad_fn=<AddBackward0>) tensor(155.8050, grad_fn=<MeanBackward0>) tensor(429.4543, grad_fn=<SumBackward0>)\n",
      "tensor(0.0244, grad_fn=<AddBackward0>) tensor(153.1701, grad_fn=<MeanBackward0>) tensor(425.6722, grad_fn=<SumBackward0>)\n",
      "Epoch 23/50 — avg train loss: 95.0982\n",
      "tensor(0.0239, grad_fn=<AddBackward0>) tensor(151.2112, grad_fn=<MeanBackward0>) tensor(421.8763, grad_fn=<SumBackward0>)\n",
      "tensor(0.0270, grad_fn=<AddBackward0>) tensor(164.2841, grad_fn=<MeanBackward0>) tensor(418.0727, grad_fn=<SumBackward0>)\n",
      "tensor(0.0225, grad_fn=<AddBackward0>) tensor(144.1559, grad_fn=<MeanBackward0>) tensor(414.2687, grad_fn=<SumBackward0>)\n",
      "Epoch 24/50 — avg train loss: 93.6352\n",
      "tensor(0.0221, grad_fn=<AddBackward0>) tensor(142.7308, grad_fn=<MeanBackward0>) tensor(410.4689, grad_fn=<SumBackward0>)\n",
      "tensor(0.0223, grad_fn=<AddBackward0>) tensor(143.3831, grad_fn=<MeanBackward0>) tensor(406.6789, grad_fn=<SumBackward0>)\n",
      "tensor(0.0229, grad_fn=<AddBackward0>) tensor(146.0255, grad_fn=<MeanBackward0>) tensor(402.9043, grad_fn=<SumBackward0>)\n",
      "Epoch 25/50 — avg train loss: 88.6846\n",
      "tensor(0.0213, grad_fn=<AddBackward0>) tensor(139.2097, grad_fn=<MeanBackward0>) tensor(399.1503, grad_fn=<SumBackward0>)\n",
      "tensor(0.0216, grad_fn=<AddBackward0>) tensor(141.2351, grad_fn=<MeanBackward0>) tensor(395.4218, grad_fn=<SumBackward0>)\n",
      "tensor(0.0209, grad_fn=<AddBackward0>) tensor(137.1625, grad_fn=<MeanBackward0>) tensor(391.7236, grad_fn=<SumBackward0>)\n",
      "Epoch 26/50 — avg train loss: 85.8183\n",
      "tensor(0.0198, grad_fn=<AddBackward0>) tensor(132.6856, grad_fn=<MeanBackward0>) tensor(388.0602, grad_fn=<SumBackward0>)\n",
      "tensor(0.0200, grad_fn=<AddBackward0>) tensor(133.5807, grad_fn=<MeanBackward0>) tensor(384.4357, grad_fn=<SumBackward0>)\n",
      "tensor(0.0202, grad_fn=<AddBackward0>) tensor(134.7305, grad_fn=<MeanBackward0>) tensor(380.8529, grad_fn=<SumBackward0>)\n",
      "Epoch 27/50 — avg train loss: 82.6323\n",
      "tensor(0.0198, grad_fn=<AddBackward0>) tensor(131.9805, grad_fn=<MeanBackward0>) tensor(377.3145, grad_fn=<SumBackward0>)\n",
      "tensor(0.0192, grad_fn=<AddBackward0>) tensor(130.8530, grad_fn=<MeanBackward0>) tensor(373.8232, grad_fn=<SumBackward0>)\n",
      "tensor(0.0187, grad_fn=<AddBackward0>) tensor(128.0573, grad_fn=<MeanBackward0>) tensor(370.3820, grad_fn=<SumBackward0>)\n",
      "Epoch 28/50 — avg train loss: 80.5053\n",
      "tensor(0.0191, grad_fn=<AddBackward0>) tensor(129.6758, grad_fn=<MeanBackward0>) tensor(366.9930, grad_fn=<SumBackward0>)\n",
      "tensor(0.0182, grad_fn=<AddBackward0>) tensor(125.9514, grad_fn=<MeanBackward0>) tensor(363.6577, grad_fn=<SumBackward0>)\n",
      "tensor(0.0176, grad_fn=<AddBackward0>) tensor(123.1835, grad_fn=<MeanBackward0>) tensor(360.3777, grad_fn=<SumBackward0>)\n",
      "Epoch 29/50 — avg train loss: 78.0837\n",
      "tensor(0.0176, grad_fn=<AddBackward0>) tensor(123.1984, grad_fn=<MeanBackward0>) tensor(357.1542, grad_fn=<SumBackward0>)\n",
      "tensor(0.0173, grad_fn=<AddBackward0>) tensor(121.6926, grad_fn=<MeanBackward0>) tensor(353.9880, grad_fn=<SumBackward0>)\n",
      "tensor(0.0175, grad_fn=<AddBackward0>) tensor(122.5760, grad_fn=<MeanBackward0>) tensor(350.8796, grad_fn=<SumBackward0>)\n",
      "Epoch 30/50 — avg train loss: 75.8038\n",
      "tensor(0.0170, grad_fn=<AddBackward0>) tensor(120.9206, grad_fn=<MeanBackward0>) tensor(347.8294, grad_fn=<SumBackward0>)\n",
      "tensor(0.0165, grad_fn=<AddBackward0>) tensor(118.1623, grad_fn=<MeanBackward0>) tensor(344.8373, grad_fn=<SumBackward0>)\n",
      "tensor(0.0165, grad_fn=<AddBackward0>) tensor(118.3498, grad_fn=<MeanBackward0>) tensor(341.9050, grad_fn=<SumBackward0>)\n",
      "Epoch 31/50 — avg train loss: 73.7584\n",
      "tensor(0.0164, grad_fn=<AddBackward0>) tensor(117.7962, grad_fn=<MeanBackward0>) tensor(339.0323, grad_fn=<SumBackward0>)\n",
      "tensor(0.0162, grad_fn=<AddBackward0>) tensor(117.3088, grad_fn=<MeanBackward0>) tensor(336.2192, grad_fn=<SumBackward0>)\n",
      "tensor(0.0170, grad_fn=<AddBackward0>) tensor(119.8433, grad_fn=<MeanBackward0>) tensor(333.4654, grad_fn=<SumBackward0>)\n",
      "Epoch 32/50 — avg train loss: 72.9470\n",
      "tensor(0.0153, grad_fn=<AddBackward0>) tensor(113.4589, grad_fn=<MeanBackward0>) tensor(330.7701, grad_fn=<SumBackward0>)\n",
      "tensor(0.0154, grad_fn=<AddBackward0>) tensor(113.0238, grad_fn=<MeanBackward0>) tensor(328.1322, grad_fn=<SumBackward0>)\n",
      "tensor(0.0163, grad_fn=<AddBackward0>) tensor(117.4268, grad_fn=<MeanBackward0>) tensor(325.5507, grad_fn=<SumBackward0>)\n",
      "Epoch 33/50 — avg train loss: 70.7918\n",
      "tensor(0.0156, grad_fn=<AddBackward0>) tensor(114.5021, grad_fn=<MeanBackward0>) tensor(323.0243, grad_fn=<SumBackward0>)\n",
      "tensor(0.0151, grad_fn=<AddBackward0>) tensor(112.3106, grad_fn=<MeanBackward0>) tensor(320.5524, grad_fn=<SumBackward0>)\n",
      "tensor(0.0153, grad_fn=<AddBackward0>) tensor(112.0118, grad_fn=<MeanBackward0>) tensor(318.1335, grad_fn=<SumBackward0>)\n",
      "Epoch 34/50 — avg train loss: 69.6140\n",
      "tensor(0.0153, grad_fn=<AddBackward0>) tensor(111.7412, grad_fn=<MeanBackward0>) tensor(315.7675, grad_fn=<SumBackward0>)\n",
      "tensor(0.0149, grad_fn=<AddBackward0>) tensor(111.2380, grad_fn=<MeanBackward0>) tensor(313.4539, grad_fn=<SumBackward0>)\n",
      "tensor(0.0142, grad_fn=<AddBackward0>) tensor(108.0830, grad_fn=<MeanBackward0>) tensor(311.1911, grad_fn=<SumBackward0>)\n",
      "Epoch 35/50 — avg train loss: 68.0308\n",
      "tensor(0.0142, grad_fn=<AddBackward0>) tensor(107.7082, grad_fn=<MeanBackward0>) tensor(308.9784, grad_fn=<SumBackward0>)\n",
      "tensor(0.0136, grad_fn=<AddBackward0>) tensor(105.7708, grad_fn=<MeanBackward0>) tensor(306.8140, grad_fn=<SumBackward0>)\n",
      "tensor(0.0140, grad_fn=<AddBackward0>) tensor(106.6584, grad_fn=<MeanBackward0>) tensor(304.6988, grad_fn=<SumBackward0>)\n",
      "Epoch 36/50 — avg train loss: 65.9633\n",
      "tensor(0.0134, grad_fn=<AddBackward0>) tensor(103.7258, grad_fn=<MeanBackward0>) tensor(302.6296, grad_fn=<SumBackward0>)\n",
      "tensor(0.0137, grad_fn=<AddBackward0>) tensor(105.0068, grad_fn=<MeanBackward0>) tensor(300.6047, grad_fn=<SumBackward0>)\n",
      "tensor(0.0134, grad_fn=<AddBackward0>) tensor(103.6707, grad_fn=<MeanBackward0>) tensor(298.6235, grad_fn=<SumBackward0>)\n",
      "Epoch 37/50 — avg train loss: 64.4272\n",
      "tensor(0.0133, grad_fn=<AddBackward0>) tensor(103.3090, grad_fn=<MeanBackward0>) tensor(296.6857, grad_fn=<SumBackward0>)\n",
      "tensor(0.0127, grad_fn=<AddBackward0>) tensor(102.0990, grad_fn=<MeanBackward0>) tensor(294.7964, grad_fn=<SumBackward0>)\n",
      "tensor(0.0133, grad_fn=<AddBackward0>) tensor(111.5939, grad_fn=<MeanBackward0>) tensor(292.9684, grad_fn=<SumBackward0>)\n",
      "Epoch 38/50 — avg train loss: 64.8841\n",
      "tensor(0.0127, grad_fn=<AddBackward0>) tensor(119.9634, grad_fn=<MeanBackward0>) tensor(291.2292, grad_fn=<SumBackward0>)\n",
      "tensor(0.0132, grad_fn=<AddBackward0>) tensor(104.6983, grad_fn=<MeanBackward0>) tensor(289.5430, grad_fn=<SumBackward0>)\n",
      "tensor(0.0127, grad_fn=<AddBackward0>) tensor(109.1584, grad_fn=<MeanBackward0>) tensor(287.8881, grad_fn=<SumBackward0>)\n",
      "Epoch 39/50 — avg train loss: 67.3221\n",
      "tensor(0.0124, grad_fn=<AddBackward0>) tensor(98.2656, grad_fn=<MeanBackward0>) tensor(286.2646, grad_fn=<SumBackward0>)\n",
      "tensor(0.0128, grad_fn=<AddBackward0>) tensor(104.4044, grad_fn=<MeanBackward0>) tensor(284.6678, grad_fn=<SumBackward0>)\n",
      "tensor(0.0131, grad_fn=<AddBackward0>) tensor(105.4058, grad_fn=<MeanBackward0>) tensor(283.0992, grad_fn=<SumBackward0>)\n",
      "Epoch 40/50 — avg train loss: 62.9689\n",
      "tensor(0.0121, grad_fn=<AddBackward0>) tensor(97.7471, grad_fn=<MeanBackward0>) tensor(281.5588, grad_fn=<SumBackward0>)\n",
      "tensor(0.0122, grad_fn=<AddBackward0>) tensor(98.5376, grad_fn=<MeanBackward0>) tensor(280.0471, grad_fn=<SumBackward0>)\n",
      "tensor(0.0120, grad_fn=<AddBackward0>) tensor(98.4045, grad_fn=<MeanBackward0>) tensor(278.5640, grad_fn=<SumBackward0>)\n",
      "Epoch 41/50 — avg train loss: 60.6046\n",
      "tensor(0.0117, grad_fn=<AddBackward0>) tensor(98.1847, grad_fn=<MeanBackward0>) tensor(277.1091, grad_fn=<SumBackward0>)\n",
      "tensor(0.0123, grad_fn=<AddBackward0>) tensor(98.3753, grad_fn=<MeanBackward0>) tensor(275.6827, grad_fn=<SumBackward0>)\n",
      "tensor(0.0126, grad_fn=<AddBackward0>) tensor(98.9874, grad_fn=<MeanBackward0>) tensor(274.2843, grad_fn=<SumBackward0>)\n",
      "Epoch 42/50 — avg train loss: 60.5325\n",
      "tensor(0.0115, grad_fn=<AddBackward0>) tensor(94.7664, grad_fn=<MeanBackward0>) tensor(272.9139, grad_fn=<SumBackward0>)\n",
      "tensor(0.0118, grad_fn=<AddBackward0>) tensor(96.0745, grad_fn=<MeanBackward0>) tensor(271.5714, grad_fn=<SumBackward0>)\n",
      "tensor(0.0123, grad_fn=<AddBackward0>) tensor(98.0272, grad_fn=<MeanBackward0>) tensor(270.2565, grad_fn=<SumBackward0>)\n",
      "Epoch 43/50 — avg train loss: 59.2661\n",
      "tensor(0.0113, grad_fn=<AddBackward0>) tensor(94.2748, grad_fn=<MeanBackward0>) tensor(268.9691, grad_fn=<SumBackward0>)\n",
      "tensor(0.0116, grad_fn=<AddBackward0>) tensor(94.8953, grad_fn=<MeanBackward0>) tensor(267.7079, grad_fn=<SumBackward0>)\n",
      "tensor(0.0113, grad_fn=<AddBackward0>) tensor(93.3128, grad_fn=<MeanBackward0>) tensor(266.4719, grad_fn=<SumBackward0>)\n",
      "Epoch 44/50 — avg train loss: 58.0586\n",
      "tensor(0.0121, grad_fn=<AddBackward0>) tensor(96.1377, grad_fn=<MeanBackward0>) tensor(265.2607, grad_fn=<SumBackward0>)\n",
      "tensor(0.0115, grad_fn=<AddBackward0>) tensor(94.0404, grad_fn=<MeanBackward0>) tensor(264.0742, grad_fn=<SumBackward0>)\n",
      "tensor(0.0107, grad_fn=<AddBackward0>) tensor(90.9809, grad_fn=<MeanBackward0>) tensor(262.9100, grad_fn=<SumBackward0>)\n",
      "Epoch 45/50 — avg train loss: 57.6723\n",
      "tensor(0.0115, grad_fn=<AddBackward0>) tensor(93.5935, grad_fn=<MeanBackward0>) tensor(261.7684, grad_fn=<SumBackward0>)\n",
      "tensor(0.0110, grad_fn=<AddBackward0>) tensor(92.0127, grad_fn=<MeanBackward0>) tensor(260.6486, grad_fn=<SumBackward0>)\n",
      "tensor(0.0105, grad_fn=<AddBackward0>) tensor(89.9221, grad_fn=<MeanBackward0>) tensor(259.5492, grad_fn=<SumBackward0>)\n",
      "Epoch 46/50 — avg train loss: 56.6065\n",
      "tensor(0.0107, grad_fn=<AddBackward0>) tensor(90.8481, grad_fn=<MeanBackward0>) tensor(258.4692, grad_fn=<SumBackward0>)\n",
      "tensor(0.0108, grad_fn=<AddBackward0>) tensor(90.9694, grad_fn=<MeanBackward0>) tensor(257.4080, grad_fn=<SumBackward0>)\n",
      "tensor(0.0106, grad_fn=<AddBackward0>) tensor(89.8257, grad_fn=<MeanBackward0>) tensor(256.3647, grad_fn=<SumBackward0>)\n",
      "Epoch 47/50 — avg train loss: 55.8290\n",
      "tensor(0.0103, grad_fn=<AddBackward0>) tensor(88.4922, grad_fn=<MeanBackward0>) tensor(255.3390, grad_fn=<SumBackward0>)\n",
      "tensor(0.0107, grad_fn=<AddBackward0>) tensor(89.7853, grad_fn=<MeanBackward0>) tensor(254.3305, grad_fn=<SumBackward0>)\n",
      "tensor(0.0102, grad_fn=<AddBackward0>) tensor(87.6397, grad_fn=<MeanBackward0>) tensor(253.3389, grad_fn=<SumBackward0>)\n",
      "Epoch 48/50 — avg train loss: 54.7649\n",
      "tensor(0.0101, grad_fn=<AddBackward0>) tensor(87.4685, grad_fn=<MeanBackward0>) tensor(252.3628, grad_fn=<SumBackward0>)\n",
      "tensor(0.0105, grad_fn=<AddBackward0>) tensor(88.5367, grad_fn=<MeanBackward0>) tensor(251.4016, grad_fn=<SumBackward0>)\n",
      "tensor(0.0104, grad_fn=<AddBackward0>) tensor(87.5609, grad_fn=<MeanBackward0>) tensor(250.4547, grad_fn=<SumBackward0>)\n",
      "Epoch 49/50 — avg train loss: 54.2480\n",
      "tensor(0.0099, grad_fn=<AddBackward0>) tensor(85.6070, grad_fn=<MeanBackward0>) tensor(249.5213, grad_fn=<SumBackward0>)\n",
      "tensor(0.0110, grad_fn=<AddBackward0>) tensor(89.2416, grad_fn=<MeanBackward0>) tensor(248.6007, grad_fn=<SumBackward0>)\n",
      "tensor(0.0102, grad_fn=<AddBackward0>) tensor(87.0660, grad_fn=<MeanBackward0>) tensor(247.6921, grad_fn=<SumBackward0>)\n",
      "Epoch 50/50 — avg train loss: 53.8493\n",
      "Detected anomalies in test set: 33 / 120\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %% [code]\n",
    "# 4) Model + optimizer\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_features = 3 * 64 * 64\n",
    "\n",
    "model = DAGMM(\n",
    "    input_dim        = 3 * 64 * 64,     # still required by signature but not forwarded to CompressionNetwork\n",
    "    latent_dim       = 90,\n",
    "    n_gmm_components = 6,\n",
    "    comp_kwargs      = {'latent_dim': 90},  # now cleanly matches CompressionNetwork\n",
    "    est_kwargs       = {'hidden_dims': [128], 'activation': torch.nn.Tanh, 'dropout': 0.3},\n",
    "    device           = device\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# %% [code]\n",
    "# 5) Training loop\n",
    "n_epochs = 50\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        x = imgs.to(device) \n",
    "        out  = model(x)\n",
    "        loss = model.loss_function(x, out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}/{n_epochs} — avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# %% [code]\n",
    "# 6) Scoring test set & thresholding\n",
    "model.eval()\n",
    "energies = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in test_loader:\n",
    "        x = imgs.to(device)\n",
    "        energies.append(model(x)['energy'].cpu())\n",
    "energies = torch.cat(energies)\n",
    "\n",
    "# 95th‐percentile threshold\n",
    "thr = energies.quantile(0.70)\n",
    "mask = energies > thr\n",
    "print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# %% [code]\n",
    "# 7) (Optional) Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "plt.axvline(thr, color='r', linestyle='--', label='66% threshold')\n",
    "plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f89b9b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 664.6502,  664.6502, 1473.0531, 1468.5046, 1447.4045, 1413.5607,\n",
      "         664.6502, 1406.5906,  664.6502, 1439.2640,  664.6502,  664.6502,\n",
      "        1467.5956, 1477.5927,  664.6502,  664.6502,  664.6502,  664.6502,\n",
      "         664.6502,  664.6502,   88.0706,   88.0706,   81.5525,   93.1890,\n",
      "          81.5525,   84.8126,   81.5525,   96.4782,   88.0706,   84.8126,\n",
      "          96.4782,   96.4782,   83.6640,   84.8126,   88.0706,   83.6640,\n",
      "          81.5525,   88.0706,   84.8126,   81.5525,   81.5525,   96.4782,\n",
      "          88.0706,   83.6640,   88.0706,   88.0706,   88.0706,   96.4782,\n",
      "          81.5525,   81.5525,   93.1890,   93.1890,   96.4782,   96.4782,\n",
      "          81.5525,   93.1890,   88.0706,   84.8126,   81.5525,   84.8126,\n",
      "          83.6640,   88.0706,   88.0706,   84.8126,   84.8126,   93.1890,\n",
      "          84.8126,   84.8126,   84.8126,   93.1890,   96.4782,   84.8126,\n",
      "          81.5525,   96.4782,   84.8126,   96.4782,   81.5525,   93.1890,\n",
      "          88.0706,   84.8126,   81.5525,   81.5525,   93.1890,   81.5525,\n",
      "          83.6640,   83.6640,   84.8126,   83.6640,   88.0706,   81.5525,\n",
      "          84.8126,   93.1890,   83.6640,   84.8126,   81.5525,   83.6640,\n",
      "          96.4782,   88.0706,   96.4782,   88.0706,   81.5525,   81.5525,\n",
      "          83.6640,   83.6640,   88.0706,   84.8126,   84.8126,   93.1890,\n",
      "          93.1890,   88.0706,   96.4782,   93.1890,   84.8126,   84.8126,\n",
      "          93.1890,   83.6640,   88.0706,   88.0706,   81.5525,   84.8126])\n",
      "tensor([False, False,  True,  True,  True,  True, False,  True, False,  True,\n",
      "        False, False,  True,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Detected anomalies in test set: 8 / 120\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# 6) Scoring test set & thresholding\n",
    "model.eval()\n",
    "energies = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in test_loader:\n",
    "        x = imgs.to(device)\n",
    "        energies.append(model(x)['energy'].cpu())\n",
    "energies = torch.cat(energies)\n",
    "\n",
    "# 95th‐percentile threshold\n",
    "thr = energies.quantile(0.90)\n",
    "mask = energies > thr\n",
    "print(energies)\n",
    "print(mask)\n",
    "print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# # %% [code]\n",
    "# # 7) (Optional) Visualize\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "# plt.axvline(thr, color='r', linestyle='--', label='66% threshold')\n",
    "# plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee114b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for top 30% anomalies: 96.47823333740234\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  0]\n",
      " [13 87]]\n",
      "\n",
      "Accuracy: 89.17%\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   anomalous       0.61      1.00      0.75        20\n",
      "      normal       1.00      0.87      0.93       100\n",
      "\n",
      "    accuracy                           0.89       120\n",
      "   macro avg       0.80      0.94      0.84       120\n",
      "weighted avg       0.93      0.89      0.90       120\n",
      "\n",
      "\n",
      "ROC‑AUC (energy as score): 1.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# 1) Recompute energies and collect true labels\n",
    "model.eval()\n",
    "energies = []\n",
    "y_true   = []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        x = imgs.to(device)                       # CNN takes [B,3,64,64]\n",
    "        out = model(x)\n",
    "        energies.append(out['energy'].cpu())      # [B]\n",
    "        y_true.append(labels)\n",
    "energies = torch.cat(energies)                  # [N_test]\n",
    "y_true   = torch.cat(y_true)                    # [N_test]\n",
    "\n",
    "# 2) Identify top 30% highest‐energy samples as anomalies\n",
    "#    70th percentile cutoff → top 30% above this\n",
    "thr = energies.quantile(0.80)\n",
    "\n",
    "\n",
    "# 3) Predictions\n",
    "# 3) Logical not then cast\n",
    "y_pred_inv = (~(energies > thr)).int() #we want 1 for normal, 0 for anomaly. High energy = anomaly. \n",
    "\n",
    "# 4) Metrics\n",
    "print(\"Threshold for top 30% anomalies:\", thr.item())\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "acc = (y_true == y_pred).float().mean() * 100\n",
    "print(f\"\\nAccuracy: {acc:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_ds.classes))\n",
    "\n",
    "# 5) ROC‑AUC (still informative even though we fix the cutoff by proportion)\n",
    "auc = roc_auc_score(y_true, -energies)  # invert since lower energy = more normal\n",
    "print(f\"\\nROC‑AUC (energy as score): {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3bbdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
