{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547ef3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bbfd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = train_dataset.classes  # ['anomaly', 'normal']\n",
    "\n",
    "# data_iter = iter(train_loader)\n",
    "# images, labels = next(data_iter)\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# for i in range(16):\n",
    "#     plt.subplot(4, 4, i + 1)\n",
    "#     img = images[i].permute(1, 2, 0)\n",
    "#     label = class_names[labels[i]]\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(label)\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb21f87",
   "metadata": {},
   "source": [
    "# Method 1, flatten the input as a long array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805bc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Check folder contents\n",
    "# import os\n",
    "\n",
    "# print(\"Train split:\")\n",
    "# for cls in (\"normal\",\"anomaly\"):\n",
    "#     p = f\"../split_anomaly_dataset/train/{cls}\"\n",
    "#     cnt = len([f for f in os.listdir(p) if f.lower().endswith(('.jpg','.png'))]) \\\n",
    "#           if os.path.isdir(p) else 0\n",
    "#     print(f\"  {cls:7s}: {cnt} files\")\n",
    "\n",
    "# # 2) Load with allow_empty\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# tf = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=tf, allow_empty=True)\n",
    "# print(\"Classes loaded:\", train_ds.classes)\n",
    "# print(\"Number of samples:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2655d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [markdown]\n",
    "# # # DAGMM Anomaly Detection with Sampled Anomalies in Train\n",
    "\n",
    "# # %% [code]\n",
    "# # 1) Make your DAGMM code importable\n",
    "# import sys, os\n",
    "# from pathlib import Path\n",
    "# sys.path.insert(0, str(Path.cwd().parent / \"Appropriate_dagmm\"))\n",
    "\n",
    "# # %% [code]\n",
    "# # 2) Imports\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms, datasets\n",
    "# from collections import Counter\n",
    "\n",
    "# from model import DAGMM  # your revised model.py\n",
    "\n",
    "# # %% [code]\n",
    "# # 3) Data transforms & loaders (images → flattened vectors)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((64,64)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "# test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# # Print ground‑truth counts\n",
    "# print(\"Train class counts:\", {train_ds.classes[k]: v for k,v in Counter(train_ds.targets).items()})\n",
    "# print(\"Test  class counts:\", {test_ds.classes[k]: v for k,v in Counter(test_ds.targets).items()})\n",
    "\n",
    "# # %% [code]\n",
    "# # 4) Model + optimizer\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_features = 3 * 64 * 64\n",
    "\n",
    "# model = DAGMM(\n",
    "#     input_dim        = n_features,\n",
    "#     latent_dim       = 10,\n",
    "#     n_gmm_components = 5,\n",
    "#     comp_kwargs      = {'hidden_dims':[128,64], 'activation':torch.nn.Tanh},\n",
    "#     est_kwargs       = {'hidden_dims':[32],      'activation':torch.nn.ReLU, 'dropout':0.3},\n",
    "#     device           = device\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # %% [code]\n",
    "# # 5) Training loop\n",
    "# n_epochs = 30\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for imgs, _ in train_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)  # flatten\n",
    "#         out  = model(x)\n",
    "#         loss = model.loss_function(x, out)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * x.size(0)\n",
    "#     avg_loss = running_loss / len(train_loader.dataset)\n",
    "#     print(f\"Epoch {epoch}/{n_epochs} — avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # %% [code]\n",
    "# # 6) Scoring test set & thresholding\n",
    "# model.eval()\n",
    "# energies = []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, _ in test_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)\n",
    "#         energies.append(model(x)['energy'].cpu())\n",
    "# energies = torch.cat(energies)\n",
    "\n",
    "# # 95th‐percentile threshold\n",
    "# thr = energies.quantile(0.80)\n",
    "# mask = energies > thr\n",
    "# print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# # %% [code]\n",
    "# # 7) (Optional) Visualize\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "# plt.axvline(thr, color='r', linestyle='--', label='95% threshold')\n",
    "# plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760cc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# # 1) Recompute energies **and** collect true labels in the same order\n",
    "# model.eval()\n",
    "# energies = []\n",
    "# y_true   = []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, labels in test_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)\n",
    "#         energies.append(model(x)['energy'].cpu())\n",
    "#         y_true.append(labels)\n",
    "# energies = torch.cat(energies)         # shape [N_test]\n",
    "# y_true   = torch.cat(y_true)           # shape [N_test]\n",
    "\n",
    "# # 2) Choose a threshold (you could sweep this on a val set)\n",
    "# thr = energies.quantile(0.95)\n",
    "\n",
    "# # 3) Build binary predictions: 1=anomaly, 0=normal\n",
    "# y_pred = (energies > thr).int()\n",
    "\n",
    "# # 4) Confusion matrix & classification report\n",
    "# print(\"Confusion matrix:\")\n",
    "# print(confusion_matrix(y_true, y_pred))\n",
    "# print(\"Accuracy\")\n",
    "# print(f\"Accuracy: {100 * (y_true == y_pred).float().mean():.2f}%\")\n",
    "# print(\"\\nClassification report:\")\n",
    "# print(classification_report(y_true, y_pred, target_names=test_ds.classes))\n",
    "\n",
    "# # 5) (Optional) AUC of the energy scores\n",
    "# auc = roc_auc_score(y_true, -energies)  # we invert since lower energy = more normal\n",
    "# print(f\"\\nROC‑AUC (energy as score): {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cadce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get top‑k highest‑energy samples\n",
    "# k = 5\n",
    "# idx = torch.topk(energies, k=k).indices\n",
    "\n",
    "# # grab their file paths & labels\n",
    "# for i in idx:\n",
    "#     path, label = test_ds.samples[i]\n",
    "#     print(f\"{path}  →  label={test_ds.classes[label]}, energy={energies[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc0910",
   "metadata": {},
   "source": [
    "# Method 2, use cnn based architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a840fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompressionNetwork from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/compression_network.py\n",
      "DAGMM   from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/model.py\n"
     ]
    }
   ],
   "source": [
    "# 0) Point to your CNN‑DAGMM folder *before* any imports\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "cnn_dir = Path.cwd().parent / \"cnn_dagmm\"\n",
    "sys.path.insert(0, str(cnn_dir))\n",
    "\n",
    "# 1) Import & force‑reload to clear any old cache\n",
    "import compression_network, model\n",
    "importlib.reload(compression_network)\n",
    "importlib.reload(model)\n",
    "\n",
    "# 2) Verify you’re using the right files\n",
    "print(\"CompressionNetwork from:\", compression_network.__file__)\n",
    "print(\"DAGMM   from:\",           model.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25f81c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: {'normal': 284}\n",
      "Test  class counts: {'anomaly': 20, 'normal': 95}\n",
      "Epoch 1/30 — avg train loss: 36.6269\n",
      "Epoch 2/30 — avg train loss: 21.1110\n",
      "Epoch 3/30 — avg train loss: 16.0705\n",
      "Epoch 4/30 — avg train loss: 14.5800\n",
      "Epoch 5/30 — avg train loss: 13.6081\n",
      "Epoch 6/30 — avg train loss: 13.1374\n",
      "Epoch 7/30 — avg train loss: 12.6400\n",
      "Epoch 8/30 — avg train loss: 12.4542\n",
      "Epoch 9/30 — avg train loss: 12.0915\n",
      "Epoch 10/30 — avg train loss: 11.4879\n",
      "Epoch 11/30 — avg train loss: 11.2674\n",
      "Epoch 12/30 — avg train loss: 10.8162\n",
      "Epoch 13/30 — avg train loss: 10.4227\n",
      "Epoch 14/30 — avg train loss: 10.2395\n",
      "Epoch 15/30 — avg train loss: 9.8681\n",
      "Epoch 16/30 — avg train loss: 9.4231\n",
      "Epoch 17/30 — avg train loss: 9.1512\n",
      "Epoch 18/30 — avg train loss: 9.0032\n",
      "Epoch 19/30 — avg train loss: 8.9751\n",
      "Epoch 20/30 — avg train loss: 8.6697\n",
      "Epoch 21/30 — avg train loss: 8.3703\n",
      "Epoch 22/30 — avg train loss: 8.2131\n",
      "Epoch 23/30 — avg train loss: 8.0712\n",
      "Epoch 24/30 — avg train loss: 7.8517\n",
      "Epoch 25/30 — avg train loss: 7.8492\n",
      "Epoch 26/30 — avg train loss: 7.6475\n",
      "Epoch 27/30 — avg train loss: 7.3021\n",
      "Epoch 28/30 — avg train loss: 7.1747\n",
      "Epoch 29/30 — avg train loss: 7.1570\n",
      "Epoch 30/30 — avg train loss: 6.9926\n",
      "Detected anomalies in test set: 35 / 115\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # DAGMM Anomaly Detection with Sampled Anomalies in Train\n",
    "\n",
    "# %% [code]\n",
    "# 1) Make your DAGMM code importable\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"cnn_dagmm\"))\n",
    "\n",
    "# %% [code]\n",
    "# 2) Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from collections import Counter\n",
    "\n",
    "from model import DAGMM  # your revised model.py\n",
    "\n",
    "# %% [code]\n",
    "# 3) Data transforms & loaders (images → flattened vectors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# Print ground‑truth counts\n",
    "print(\"Train class counts:\", {train_ds.classes[k]: v for k,v in Counter(train_ds.targets).items()})\n",
    "print(\"Test  class counts:\", {test_ds.classes[k]: v for k,v in Counter(test_ds.targets).items()})\n",
    "\n",
    "# %% [code]\n",
    "# 4) Model + optimizer\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_features = 3 * 64 * 64\n",
    "\n",
    "model = DAGMM(\n",
    "    input_dim        = 3 * 64 * 64,     # still required by signature but not forwarded to CompressionNetwork\n",
    "    latent_dim       = 2,\n",
    "    n_gmm_components = 2,\n",
    "    comp_kwargs      = {'latent_dim': 2},  # now cleanly matches CompressionNetwork\n",
    "    est_kwargs       = {'hidden_dims': [32], 'activation': torch.nn.ReLU, 'dropout': 0.3},\n",
    "    device           = device\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# %% [code]\n",
    "# 5) Training loop\n",
    "n_epochs = 30\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        x = imgs.to(device) \n",
    "        out  = model(x)\n",
    "        loss = model.loss_function(x, out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}/{n_epochs} — avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# %% [code]\n",
    "# 6) Scoring test set & thresholding\n",
    "model.eval()\n",
    "energies = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in test_loader:\n",
    "        x = imgs.to(device)\n",
    "        energies.append(model(x)['energy'].cpu())\n",
    "energies = torch.cat(energies)\n",
    "\n",
    "# 95th‐percentile threshold\n",
    "thr = energies.quantile(0.70)\n",
    "mask = energies > thr\n",
    "print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# %% [code]\n",
    "# 7) (Optional) Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "plt.axvline(thr, color='r', linestyle='--', label='66% threshold')\n",
    "plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e7a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for top 30% anomalies: 193.5363006591797\n",
      "\n",
      "Confusion matrix:\n",
      "[[15  5]\n",
      " [94  1]]\n",
      "\n",
      "Accuracy: 13.91%\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anomaly       0.14      0.75      0.23        20\n",
      "      normal       0.17      0.01      0.02        95\n",
      "\n",
      "    accuracy                           0.14       115\n",
      "   macro avg       0.15      0.38      0.13       115\n",
      "weighted avg       0.16      0.14      0.06       115\n",
      "\n",
      "\n",
      "ROC‑AUC (energy as score): 0.831\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# 1) Recompute energies and collect true labels\n",
    "model.eval()\n",
    "energies = []\n",
    "y_true   = []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        x = imgs.to(device)                       # CNN takes [B,3,64,64]\n",
    "        out = model(x)\n",
    "        energies.append(out['energy'].cpu())      # [B]\n",
    "        y_true.append(labels)\n",
    "energies = torch.cat(energies)                  # [N_test]\n",
    "y_true   = torch.cat(y_true)                    # [N_test]\n",
    "\n",
    "# 2) Identify top 30% highest‐energy samples as anomalies\n",
    "#    70th percentile cutoff → top 30% above this\n",
    "thr = energies.quantile(0.66)\n",
    "\n",
    "\n",
    "# 3) Predictions\n",
    "y_pred = (energies > thr).int()\n",
    "\n",
    "# 4) Metrics\n",
    "print(\"Threshold for top 30% anomalies:\", thr.item())\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "acc = (y_true == y_pred).float().mean() * 100\n",
    "print(f\"\\nAccuracy: {acc:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_ds.classes))\n",
    "\n",
    "# 5) ROC‑AUC (still informative even though we fix the cutoff by proportion)\n",
    "auc = roc_auc_score(y_true, -energies)  # invert since lower energy = more normal\n",
    "print(f\"\\nROC‑AUC (energy as score): {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c220e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vals \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mparam_grid\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     78\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(param_grid\u001b[38;5;241m.\u001b[39mkeys(), vals))\n\u001b[0;32m---> 79\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc})\n",
      "Cell \u001b[0;32mIn[16], line 56\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     54\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     55\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_function(x, out)\n\u001b[0;32m---> 56\u001b[0m         opt\u001b[38;5;241m.\u001b[39mzero_grad(); \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m; opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# evaluate: compute energy scores & threshold at 90th percentile\u001b[39;00m\n\u001b[1;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # %% [code]\n",
    "# # Hyperparameter grid search for DAGMM\n",
    "# import itertools\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms, datasets\n",
    "# from model import DAGMM\n",
    "\n",
    "# # 1) Data loaders (reuse from above)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((64,64)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "# test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 2) Define search space\n",
    "# param_grid = {\n",
    "#     \"latent_dim\":       [2, 5, 10],\n",
    "#     \"n_gmm_components\": [2, 4, 6],\n",
    "#     \"est_hidden\":       [[32], [64, 32]],\n",
    "#     \"dropout\":          [0.1, 0.3],\n",
    "#     \"lr\":               [1e-3, 1e-4],\n",
    "# }\n",
    "\n",
    "# # 3) Helper to train & evaluate one config\n",
    "# def run_experiment(cfg):\n",
    "#     # build model\n",
    "#     model = DAGMM(\n",
    "#         input_dim        = 3*64*64,\n",
    "#         latent_dim       = cfg[\"latent_dim\"],\n",
    "#         n_gmm_components = cfg[\"n_gmm_components\"],\n",
    "#         comp_kwargs      = {\"latent_dim\": cfg[\"latent_dim\"]},\n",
    "#         est_kwargs       = {\n",
    "#             \"input_dim\": cfg[\"latent_dim\"]+2,\n",
    "#             \"output_dim\": cfg[\"n_gmm_components\"],\n",
    "#             \"hidden_dims\": cfg[\"est_hidden\"],\n",
    "#             \"activation\": torch.nn.ReLU,\n",
    "#             \"dropout\": cfg[\"dropout\"]\n",
    "#         },\n",
    "#         device=device\n",
    "#     ).to(device)\n",
    "#     opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
    "\n",
    "#     # train for fixed epochs\n",
    "#     for epoch in range(1, 11):\n",
    "#         model.train()\n",
    "#         for imgs, _ in train_loader:\n",
    "#             x = imgs.to(device)\n",
    "#             out = model(x)\n",
    "#             loss = model.loss_function(x, out)\n",
    "#             opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "#     # evaluate: compute energy scores & threshold at 90th percentile\n",
    "#     model.eval()\n",
    "#     energies, y_true = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for imgs, labels in test_loader:\n",
    "#             x = imgs.to(device)\n",
    "#             energies.append(model(x)[\"energy\"].cpu())\n",
    "#             y_true.append(labels)\n",
    "#     energies = torch.cat(energies)\n",
    "#     y_true   = torch.cat(y_true)\n",
    "\n",
    "#     thr = energies.quantile(0.9)\n",
    "#     y_pred = (energies > thr).int()\n",
    "#     acc = (y_pred == y_true).float().mean().item()\n",
    "\n",
    "#     return acc\n",
    "\n",
    "# # 4) Run grid search\n",
    "# results = []\n",
    "# for vals in itertools.product(*param_grid.values()):\n",
    "#     cfg = dict(zip(param_grid.keys(), vals))\n",
    "#     acc = run_experiment(cfg)\n",
    "#     print(f\"Config {cfg} → accuracy {acc:.3f}\")\n",
    "#     results.append({**cfg, \"accuracy\": acc})\n",
    "\n",
    "# # 5) Summarize\n",
    "# df = pd.DataFrame(results)\n",
    "# print(\"\\nTop 5 configs by accuracy:\")\n",
    "# print(df.sort_values(\"accuracy\", ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09632df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(\"accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88341822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 — avg white-loss: 5.9115\n",
      "Epoch 20 — avg white-loss: 2.1444\n",
      "Epoch 30 — avg white-loss: 1.4583\n",
      "Epoch 40 — avg white-loss: 1.0694\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 4])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m x_batch[mask]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_function(x, out)\n\u001b[1;32m     48\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/model.py:65\u001b[0m, in \u001b[0;36mDAGMM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m cos_sim \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(diff, xhat_flat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 1]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 5) Estimate mixture responsibilities on [z, recon_error, cos_sim]\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecon_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_sim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# 6) GMM features: include cos_sim as well\u001b[39;00m\n\u001b[1;32m     68\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z, recon_error, cos_sim, gamma], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/estimation_network.py:23\u001b[0m, in \u001b[0;36mEstimationNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/functional.py:2820\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2808\u001b[0m         batch_norm,\n\u001b[1;32m   2809\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2817\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2818\u001b[0m     )\n\u001b[1;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2820\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2823\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2824\u001b[0m     weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2831\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled,\n\u001b[1;32m   2832\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/torch/nn/functional.py:2786\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2784\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2786\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2787\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2788\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 4])"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from model import DAGMM\n",
    "\n",
    "# 1) Synthesize 100 images\n",
    "white_imgs   = torch.ones(80, 3, 64, 64)\n",
    "colored_imgs = torch.zeros(20, 3, 64, 64); colored_imgs[:,0] = 1.0\n",
    "labels       = torch.cat([torch.zeros(80), torch.ones(20)]).long()\n",
    "\n",
    "# single dataset\n",
    "dataset = TensorDataset(torch.cat([white_imgs, colored_imgs], 0), labels)\n",
    "\n",
    "# train loader (shuffle)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# eval loader (no shuffle)\n",
    "eval_loader  = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 2) Build & train DAGMM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DAGMM(\n",
    "    input_dim=3*64*64, latent_dim=2, n_gmm_components=2,\n",
    "    comp_kwargs={'latent_dim':2},\n",
    "    est_kwargs ={\n",
    "        'input_dim': 2+2,\n",
    "        'output_dim': 2,\n",
    "        'hidden_dims':[4],\n",
    "        'activation':torch.nn.ReLU,\n",
    "        'dropout':0.1\n",
    "    },\n",
    "    device=device\n",
    ").to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 3) Train on white only\n",
    "for epoch in range(1, 51):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_whites = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        mask = (y_batch == 0)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        x = x_batch[mask].to(device)\n",
    "        out = model(x)\n",
    "        loss = model.loss_function(x, out)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        n_whites   += x.size(0)\n",
    "\n",
    "    if epoch % 10 == 0 and n_whites > 0:\n",
    "        print(f\"Epoch {epoch:02d} — avg white-loss: {total_loss / n_whites:.4f}\")\n",
    "\n",
    "# 4) Compute energies & collect labels\n",
    "model.eval()\n",
    "energies = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in eval_loader:\n",
    "        x = x_batch.to(device)\n",
    "        out = model(x)\n",
    "        energies.append(out['energy'].cpu())\n",
    "        labels_list.append(y_batch)\n",
    "\n",
    "energies = torch.cat(energies)     # [100]\n",
    "labels   = torch.cat(labels_list)  # [100]\n",
    "\n",
    "# 5) Inspect distributions by label\n",
    "white_e = energies[labels == 0]\n",
    "red_e   = energies[labels == 1]\n",
    "\n",
    "print(\"White  μ±σ:\", white_e.mean().item(), white_e.std().item())\n",
    "print(\"Red    μ±σ:\",   red_e.mean().item(),   red_e.std().item())\n",
    "\n",
    "plt.hist(white_e, bins=20, alpha=0.6, label='white')\n",
    "plt.hist(red_e,   bins=20, alpha=0.6, label='red')\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# 6) Mid-point threshold\n",
    "thr = (white_e.mean() + red_e.mean()) / 2\n",
    "pred = (energies > thr).int()\n",
    "\n",
    "print(\"Detected anomalies:\", pred[labels==1].sum().item(), \"/ 20\")\n",
    "print(\"False positives:   \", pred[labels==0].sum().item(), \"/ 80\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "959f0c52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100, 115]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m y_score \u001b[38;5;241m=\u001b[39m energies\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 1) ROC curve and optimal threshold at max (TPR–FPR)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m opt_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(tpr \u001b[38;5;241m-\u001b[39m fpr)\n\u001b[1;32m     11\u001b[0m opt_thr \u001b[38;5;241m=\u001b[39m thresholds[opt_idx]\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1150\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1047\u001b[0m     {\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m ):\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1150\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:820\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m--> 820\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[1;32m    822\u001b[0m y_score \u001b[38;5;241m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[0;32m~/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    478\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [100, 115]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, f1_score, precision_recall_fscore_support\n",
    "\n",
    "# assume `energies` [100] and `labels` [100] (0=white,1=red)\n",
    "y_true = labels.numpy()\n",
    "y_score = energies.numpy()\n",
    "\n",
    "# 1) ROC curve and optimal threshold at max (TPR–FPR)\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "opt_idx = np.argmax(tpr - fpr)\n",
    "opt_thr = thresholds[opt_idx]\n",
    "print(f\"Optimal threshold from ROC: {opt_thr:.4f} (TPR={tpr[opt_idx]:.2f}, FPR={fpr[opt_idx]:.2f})\")\n",
    "\n",
    "# 2) Build predictions & compute metrics\n",
    "y_pred = (energies > opt_thr).int().numpy()\n",
    "tp, fp, fn, tn = (\n",
    "    ((y_true==1)&(y_pred==1)).sum(),\n",
    "    ((y_true==0)&(y_pred==1)).sum(),\n",
    "    ((y_true==1)&(y_pred==0)).sum(),\n",
    "    ((y_true==0)&(y_pred==0)).sum(),\n",
    ")\n",
    "print(\"Confusion matrix:\")\n",
    "print(f\"  [[TP={tp}, FP={fp}]\\n   [FN={fn}, TN={tn}]]\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "print(f\"\\nPrecision (white,red): {precision}\")\n",
    "print(f\"Recall    (white,red): {recall}\")\n",
    "print(f\" F1‑score (white,red): {f1}\")\n",
    "\n",
    "# 3) (Optional) print overall F1\n",
    "print(f\"\\nOverall F1: {f1_score(y_true, y_pred):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
