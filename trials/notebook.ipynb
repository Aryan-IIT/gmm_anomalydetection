{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547ef3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bbfd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = train_dataset.classes  # ['anomaly', 'normal']\n",
    "\n",
    "# data_iter = iter(train_loader)\n",
    "# images, labels = next(data_iter)\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# for i in range(16):\n",
    "#     plt.subplot(4, 4, i + 1)\n",
    "#     img = images[i].permute(1, 2, 0)\n",
    "#     label = class_names[labels[i]]\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(label)\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb21f87",
   "metadata": {},
   "source": [
    "# Method 1, flatten the input as a long array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805bc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Check folder contents\n",
    "# import os\n",
    "\n",
    "# print(\"Train split:\")\n",
    "# for cls in (\"normal\",\"anomaly\"):\n",
    "#     p = f\"../split_anomaly_dataset/train/{cls}\"\n",
    "#     cnt = len([f for f in os.listdir(p) if f.lower().endswith(('.jpg','.png'))]) \\\n",
    "#           if os.path.isdir(p) else 0\n",
    "#     print(f\"  {cls:7s}: {cnt} files\")\n",
    "\n",
    "# # 2) Load with allow_empty\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# tf = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=tf, allow_empty=True)\n",
    "# print(\"Classes loaded:\", train_ds.classes)\n",
    "# print(\"Number of samples:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2655d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [markdown]\n",
    "# # # DAGMM Anomaly Detection with Sampled Anomalies in Train\n",
    "\n",
    "# # %% [code]\n",
    "# # 1) Make your DAGMM code importable\n",
    "# import sys, os\n",
    "# from pathlib import Path\n",
    "# sys.path.insert(0, str(Path.cwd().parent / \"Appropriate_dagmm\"))\n",
    "\n",
    "# # %% [code]\n",
    "# # 2) Imports\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms, datasets\n",
    "# from collections import Counter\n",
    "\n",
    "# from model import DAGMM  # your revised model.py\n",
    "\n",
    "# # %% [code]\n",
    "# # 3) Data transforms & loaders (images → flattened vectors)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((64,64)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "# test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# # Print ground‑truth counts\n",
    "# print(\"Train class counts:\", {train_ds.classes[k]: v for k,v in Counter(train_ds.targets).items()})\n",
    "# print(\"Test  class counts:\", {test_ds.classes[k]: v for k,v in Counter(test_ds.targets).items()})\n",
    "\n",
    "# # %% [code]\n",
    "# # 4) Model + optimizer\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_features = 3 * 64 * 64\n",
    "\n",
    "# model = DAGMM(\n",
    "#     input_dim        = n_features,\n",
    "#     latent_dim       = 10,\n",
    "#     n_gmm_components = 5,\n",
    "#     comp_kwargs      = {'hidden_dims':[128,64], 'activation':torch.nn.Tanh},\n",
    "#     est_kwargs       = {'hidden_dims':[32],      'activation':torch.nn.ReLU, 'dropout':0.3},\n",
    "#     device           = device\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # %% [code]\n",
    "# # 5) Training loop\n",
    "# n_epochs = 30\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for imgs, _ in train_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)  # flatten\n",
    "#         out  = model(x)\n",
    "#         loss = model.loss_function(x, out)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * x.size(0)\n",
    "#     avg_loss = running_loss / len(train_loader.dataset)\n",
    "#     print(f\"Epoch {epoch}/{n_epochs} — avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # %% [code]\n",
    "# # 6) Scoring test set & thresholding\n",
    "# model.eval()\n",
    "# energies = []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, _ in test_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)\n",
    "#         energies.append(model(x)['energy'].cpu())\n",
    "# energies = torch.cat(energies)\n",
    "\n",
    "# # 95th‐percentile threshold\n",
    "# thr = energies.quantile(0.80)\n",
    "# mask = energies > thr\n",
    "# print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# # %% [code]\n",
    "# # 7) (Optional) Visualize\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "# plt.axvline(thr, color='r', linestyle='--', label='95% threshold')\n",
    "# plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760cc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# # 1) Recompute energies **and** collect true labels in the same order\n",
    "# model.eval()\n",
    "# energies = []\n",
    "# y_true   = []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, labels in test_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)\n",
    "#         energies.append(model(x)['energy'].cpu())\n",
    "#         y_true.append(labels)\n",
    "# energies = torch.cat(energies)         # shape [N_test]\n",
    "# y_true   = torch.cat(y_true)           # shape [N_test]\n",
    "\n",
    "# # 2) Choose a threshold (you could sweep this on a val set)\n",
    "# thr = energies.quantile(0.95)\n",
    "\n",
    "# # 3) Build binary predictions: 1=anomaly, 0=normal\n",
    "# y_pred = (energies > thr).int()\n",
    "\n",
    "# # 4) Confusion matrix & classification report\n",
    "# print(\"Confusion matrix:\")\n",
    "# print(confusion_matrix(y_true, y_pred))\n",
    "# print(\"Accuracy\")\n",
    "# print(f\"Accuracy: {100 * (y_true == y_pred).float().mean():.2f}%\")\n",
    "# print(\"\\nClassification report:\")\n",
    "# print(classification_report(y_true, y_pred, target_names=test_ds.classes))\n",
    "\n",
    "# # 5) (Optional) AUC of the energy scores\n",
    "# auc = roc_auc_score(y_true, -energies)  # we invert since lower energy = more normal\n",
    "# print(f\"\\nROC‑AUC (energy as score): {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cadce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get top‑k highest‑energy samples\n",
    "# k = 5\n",
    "# idx = torch.topk(energies, k=k).indices\n",
    "\n",
    "# # grab their file paths & labels\n",
    "# for i in idx:\n",
    "#     path, label = test_ds.samples[i]\n",
    "#     print(f\"{path}  →  label={test_ds.classes[label]}, energy={energies[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc0910",
   "metadata": {},
   "source": [
    "# Method 2, use cnn based architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a840fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompressionNetwork from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/compression_network.py\n",
      "DAGMM   from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/model.py\n"
     ]
    }
   ],
   "source": [
    "# 0) Point to your CNN‑DAGMM folder *before* any imports\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "cnn_dir = Path.cwd().parent / \"cnn_dagmm\"\n",
    "sys.path.insert(0, str(cnn_dir))\n",
    "\n",
    "# 1) Import & force‑reload to clear any old cache\n",
    "import compression_network, model\n",
    "importlib.reload(compression_network)\n",
    "importlib.reload(model)\n",
    "\n",
    "# 2) Verify you’re using the right files\n",
    "print(\"CompressionNetwork from:\", compression_network.__file__)\n",
    "print(\"DAGMM   from:\",           model.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f81c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: {'normal': 284}\n",
      "Test  class counts: {'anomaly': 20, 'normal': 95}\n",
      "tensor(0.1333, grad_fn=<AddBackward0>) tensor(666.6865, grad_fn=<MeanBackward0>) tensor(459.9996, grad_fn=<SumBackward0>)\n",
      "tensor(0.1210, grad_fn=<AddBackward0>) tensor(597.2060, grad_fn=<MeanBackward0>) tensor(459.2886, grad_fn=<SumBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>) tensor(585.0563, grad_fn=<MeanBackward0>) tensor(458.5142, grad_fn=<SumBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>) tensor(490.4244, grad_fn=<MeanBackward0>) tensor(457.7781, grad_fn=<SumBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>) tensor(461.7774, grad_fn=<MeanBackward0>) tensor(457.0681, grad_fn=<SumBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>) tensor(409.4990, grad_fn=<MeanBackward0>) tensor(456.3650, grad_fn=<SumBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>) tensor(421.2115, grad_fn=<MeanBackward0>) tensor(455.6579, grad_fn=<SumBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>) tensor(375.0172, grad_fn=<MeanBackward0>) tensor(454.9390, grad_fn=<SumBackward0>)\n",
      "Epoch 1/50 — avg train loss: 246.4500\n",
      "tensor(0.0763, grad_fn=<AddBackward0>) tensor(361.3416, grad_fn=<MeanBackward0>) tensor(454.2016, grad_fn=<SumBackward0>)\n",
      "tensor(0.0739, grad_fn=<AddBackward0>) tensor(348.8854, grad_fn=<MeanBackward0>) tensor(453.4401, grad_fn=<SumBackward0>)\n",
      "tensor(0.0708, grad_fn=<AddBackward0>) tensor(335.6079, grad_fn=<MeanBackward0>) tensor(452.6477, grad_fn=<SumBackward0>)\n",
      "tensor(0.0630, grad_fn=<AddBackward0>) tensor(292.2037, grad_fn=<MeanBackward0>) tensor(451.8180, grad_fn=<SumBackward0>)\n",
      "tensor(0.0658, grad_fn=<AddBackward0>) tensor(311.3812, grad_fn=<MeanBackward0>) tensor(450.9439, grad_fn=<SumBackward0>)\n",
      "tensor(0.0665, grad_fn=<AddBackward0>) tensor(320.9259, grad_fn=<MeanBackward0>) tensor(450.0202, grad_fn=<SumBackward0>)\n",
      "tensor(0.0656, grad_fn=<AddBackward0>) tensor(316.0449, grad_fn=<MeanBackward0>) tensor(449.0420, grad_fn=<SumBackward0>)\n",
      "tensor(0.0583, grad_fn=<AddBackward0>) tensor(279.3153, grad_fn=<MeanBackward0>) tensor(448.0048, grad_fn=<SumBackward0>)\n",
      "Epoch 2/50 — avg train loss: 164.9464\n",
      "tensor(0.0529, grad_fn=<AddBackward0>) tensor(254.0697, grad_fn=<MeanBackward0>) tensor(446.9044, grad_fn=<SumBackward0>)\n",
      "tensor(0.0570, grad_fn=<AddBackward0>) tensor(279.0046, grad_fn=<MeanBackward0>) tensor(445.7369, grad_fn=<SumBackward0>)\n",
      "tensor(0.0539, grad_fn=<AddBackward0>) tensor(265.1849, grad_fn=<MeanBackward0>) tensor(444.4992, grad_fn=<SumBackward0>)\n",
      "tensor(0.0541, grad_fn=<AddBackward0>) tensor(270.4525, grad_fn=<MeanBackward0>) tensor(443.1892, grad_fn=<SumBackward0>)\n",
      "tensor(0.0537, grad_fn=<AddBackward0>) tensor(265.0442, grad_fn=<MeanBackward0>) tensor(441.8044, grad_fn=<SumBackward0>)\n",
      "tensor(0.0488, grad_fn=<AddBackward0>) tensor(246.1744, grad_fn=<MeanBackward0>) tensor(440.3434, grad_fn=<SumBackward0>)\n",
      "tensor(0.0471, grad_fn=<AddBackward0>) tensor(234.4211, grad_fn=<MeanBackward0>) tensor(438.8053, grad_fn=<SumBackward0>)\n",
      "tensor(0.0455, grad_fn=<AddBackward0>) tensor(233.7228, grad_fn=<MeanBackward0>) tensor(437.1894, grad_fn=<SumBackward0>)\n",
      "Epoch 3/50 — avg train loss: 135.3660\n",
      "tensor(0.0460, grad_fn=<AddBackward0>) tensor(230.7407, grad_fn=<MeanBackward0>) tensor(435.4941, grad_fn=<SumBackward0>)\n",
      "tensor(0.0430, grad_fn=<AddBackward0>) tensor(219.8730, grad_fn=<MeanBackward0>) tensor(433.7213, grad_fn=<SumBackward0>)\n",
      "tensor(0.0454, grad_fn=<AddBackward0>) tensor(228.0754, grad_fn=<MeanBackward0>) tensor(431.8734, grad_fn=<SumBackward0>)\n",
      "tensor(0.0506, grad_fn=<AddBackward0>) tensor(254.7849, grad_fn=<MeanBackward0>) tensor(429.9507, grad_fn=<SumBackward0>)\n",
      "tensor(0.0416, grad_fn=<AddBackward0>) tensor(210.3594, grad_fn=<MeanBackward0>) tensor(427.9554, grad_fn=<SumBackward0>)\n",
      "tensor(0.0430, grad_fn=<AddBackward0>) tensor(218.9451, grad_fn=<MeanBackward0>) tensor(425.8885, grad_fn=<SumBackward0>)\n",
      "tensor(0.0413, grad_fn=<AddBackward0>) tensor(208.6411, grad_fn=<MeanBackward0>) tensor(423.7520, grad_fn=<SumBackward0>)\n",
      "tensor(0.0389, grad_fn=<AddBackward0>) tensor(200.3358, grad_fn=<MeanBackward0>) tensor(421.5466, grad_fn=<SumBackward0>)\n",
      "Epoch 4/50 — avg train loss: 119.1816\n",
      "tensor(0.0417, grad_fn=<AddBackward0>) tensor(212.4588, grad_fn=<MeanBackward0>) tensor(419.2745, grad_fn=<SumBackward0>)\n",
      "tensor(0.0388, grad_fn=<AddBackward0>) tensor(199.9539, grad_fn=<MeanBackward0>) tensor(416.9384, grad_fn=<SumBackward0>)\n",
      "tensor(0.0412, grad_fn=<AddBackward0>) tensor(212.1152, grad_fn=<MeanBackward0>) tensor(414.5411, grad_fn=<SumBackward0>)\n",
      "tensor(0.0384, grad_fn=<AddBackward0>) tensor(200.2079, grad_fn=<MeanBackward0>) tensor(412.0856, grad_fn=<SumBackward0>)\n",
      "tensor(0.0420, grad_fn=<AddBackward0>) tensor(216.1775, grad_fn=<MeanBackward0>) tensor(409.5755, grad_fn=<SumBackward0>)\n",
      "tensor(0.0396, grad_fn=<AddBackward0>) tensor(204.9283, grad_fn=<MeanBackward0>) tensor(407.0134, grad_fn=<SumBackward0>)\n",
      "tensor(0.0398, grad_fn=<AddBackward0>) tensor(205.6474, grad_fn=<MeanBackward0>) tensor(404.4045, grad_fn=<SumBackward0>)\n",
      "tensor(0.0416, grad_fn=<AddBackward0>) tensor(215.7338, grad_fn=<MeanBackward0>) tensor(401.7523, grad_fn=<SumBackward0>)\n",
      "Epoch 5/50 — avg train loss: 112.4748\n",
      "tensor(0.0422, grad_fn=<AddBackward0>) tensor(213.1335, grad_fn=<MeanBackward0>) tensor(399.0600, grad_fn=<SumBackward0>)\n",
      "tensor(0.0341, grad_fn=<AddBackward0>) tensor(182.3884, grad_fn=<MeanBackward0>) tensor(396.3308, grad_fn=<SumBackward0>)\n",
      "tensor(0.0383, grad_fn=<AddBackward0>) tensor(198.7559, grad_fn=<MeanBackward0>) tensor(393.5683, grad_fn=<SumBackward0>)\n",
      "tensor(0.0350, grad_fn=<AddBackward0>) tensor(185.8723, grad_fn=<MeanBackward0>) tensor(390.7767, grad_fn=<SumBackward0>)\n",
      "tensor(0.0406, grad_fn=<AddBackward0>) tensor(207.1931, grad_fn=<MeanBackward0>) tensor(387.9600, grad_fn=<SumBackward0>)\n",
      "tensor(0.0365, grad_fn=<AddBackward0>) tensor(190.7817, grad_fn=<MeanBackward0>) tensor(385.1232, grad_fn=<SumBackward0>)\n",
      "tensor(0.0381, grad_fn=<AddBackward0>) tensor(199.5056, grad_fn=<MeanBackward0>) tensor(382.2693, grad_fn=<SumBackward0>)\n",
      "tensor(0.0352, grad_fn=<AddBackward0>) tensor(184.7892, grad_fn=<MeanBackward0>) tensor(379.4020, grad_fn=<SumBackward0>)\n",
      "Epoch 6/50 — avg train loss: 105.6039\n",
      "tensor(0.0360, grad_fn=<AddBackward0>) tensor(190.4220, grad_fn=<MeanBackward0>) tensor(376.5263, grad_fn=<SumBackward0>)\n",
      "tensor(0.0378, grad_fn=<AddBackward0>) tensor(195.0042, grad_fn=<MeanBackward0>) tensor(373.6443, grad_fn=<SumBackward0>)\n",
      "tensor(0.0364, grad_fn=<AddBackward0>) tensor(186.9098, grad_fn=<MeanBackward0>) tensor(370.7603, grad_fn=<SumBackward0>)\n",
      "tensor(0.0348, grad_fn=<AddBackward0>) tensor(181.3936, grad_fn=<MeanBackward0>) tensor(367.8788, grad_fn=<SumBackward0>)\n",
      "tensor(0.0369, grad_fn=<AddBackward0>) tensor(187.4917, grad_fn=<MeanBackward0>) tensor(365.0016, grad_fn=<SumBackward0>)\n",
      "tensor(0.0351, grad_fn=<AddBackward0>) tensor(183.0586, grad_fn=<MeanBackward0>) tensor(362.1327, grad_fn=<SumBackward0>)\n",
      "tensor(0.0339, grad_fn=<AddBackward0>) tensor(176.8598, grad_fn=<MeanBackward0>) tensor(359.2750, grad_fn=<SumBackward0>)\n",
      "tensor(0.0366, grad_fn=<AddBackward0>) tensor(185.4416, grad_fn=<MeanBackward0>) tensor(356.4294, grad_fn=<SumBackward0>)\n",
      "Epoch 7/50 — avg train loss: 100.2998\n",
      "tensor(0.0357, grad_fn=<AddBackward0>) tensor(180.4126, grad_fn=<MeanBackward0>) tensor(353.6009, grad_fn=<SumBackward0>)\n",
      "tensor(0.0352, grad_fn=<AddBackward0>) tensor(183.7063, grad_fn=<MeanBackward0>) tensor(350.7923, grad_fn=<SumBackward0>)\n",
      "tensor(0.0398, grad_fn=<AddBackward0>) tensor(201.5643, grad_fn=<MeanBackward0>) tensor(348.0061, grad_fn=<SumBackward0>)\n",
      "tensor(0.0331, grad_fn=<AddBackward0>) tensor(170.5709, grad_fn=<MeanBackward0>) tensor(345.2450, grad_fn=<SumBackward0>)\n",
      "tensor(0.0356, grad_fn=<AddBackward0>) tensor(179.0080, grad_fn=<MeanBackward0>) tensor(342.5101, grad_fn=<SumBackward0>)\n",
      "tensor(0.0371, grad_fn=<AddBackward0>) tensor(189.5183, grad_fn=<MeanBackward0>) tensor(339.8030, grad_fn=<SumBackward0>)\n",
      "tensor(0.0361, grad_fn=<AddBackward0>) tensor(179.8464, grad_fn=<MeanBackward0>) tensor(337.1249, grad_fn=<SumBackward0>)\n",
      "tensor(0.0375, grad_fn=<AddBackward0>) tensor(188.5573, grad_fn=<MeanBackward0>) tensor(334.4785, grad_fn=<SumBackward0>)\n",
      "Epoch 8/50 — avg train loss: 98.5307\n",
      "tensor(0.0351, grad_fn=<AddBackward0>) tensor(176.3325, grad_fn=<MeanBackward0>) tensor(331.8643, grad_fn=<SumBackward0>)\n",
      "tensor(0.0361, grad_fn=<AddBackward0>) tensor(180.7641, grad_fn=<MeanBackward0>) tensor(329.2828, grad_fn=<SumBackward0>)\n",
      "tensor(0.0357, grad_fn=<AddBackward0>) tensor(181.3875, grad_fn=<MeanBackward0>) tensor(326.7359, grad_fn=<SumBackward0>)\n",
      "tensor(0.0342, grad_fn=<AddBackward0>) tensor(172.3719, grad_fn=<MeanBackward0>) tensor(324.2238, grad_fn=<SumBackward0>)\n",
      "tensor(0.0331, grad_fn=<AddBackward0>) tensor(169.4413, grad_fn=<MeanBackward0>) tensor(321.7490, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(166.5186, grad_fn=<MeanBackward0>) tensor(319.3110, grad_fn=<SumBackward0>)\n",
      "tensor(0.0352, grad_fn=<AddBackward0>) tensor(176.0100, grad_fn=<MeanBackward0>) tensor(316.9112, grad_fn=<SumBackward0>)\n",
      "tensor(0.0344, grad_fn=<AddBackward0>) tensor(172.0170, grad_fn=<MeanBackward0>) tensor(314.5498, grad_fn=<SumBackward0>)\n",
      "Epoch 9/50 — avg train loss: 93.1751\n",
      "tensor(0.0347, grad_fn=<AddBackward0>) tensor(170.8969, grad_fn=<MeanBackward0>) tensor(312.2269, grad_fn=<SumBackward0>)\n",
      "tensor(0.0364, grad_fn=<AddBackward0>) tensor(178.0001, grad_fn=<MeanBackward0>) tensor(309.9433, grad_fn=<SumBackward0>)\n",
      "tensor(0.0318, grad_fn=<AddBackward0>) tensor(162.8329, grad_fn=<MeanBackward0>) tensor(307.6982, grad_fn=<SumBackward0>)\n",
      "tensor(0.0341, grad_fn=<AddBackward0>) tensor(169.3074, grad_fn=<MeanBackward0>) tensor(305.4918, grad_fn=<SumBackward0>)\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(162.6906, grad_fn=<MeanBackward0>) tensor(303.3232, grad_fn=<SumBackward0>)\n",
      "tensor(0.0353, grad_fn=<AddBackward0>) tensor(176.0577, grad_fn=<MeanBackward0>) tensor(301.1921, grad_fn=<SumBackward0>)\n",
      "tensor(0.0344, grad_fn=<AddBackward0>) tensor(166.3078, grad_fn=<MeanBackward0>) tensor(299.1001, grad_fn=<SumBackward0>)\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(165.4696, grad_fn=<MeanBackward0>) tensor(297.0469, grad_fn=<SumBackward0>)\n",
      "Epoch 10/50 — avg train loss: 89.8991\n",
      "tensor(0.0375, grad_fn=<AddBackward0>) tensor(180.8948, grad_fn=<MeanBackward0>) tensor(295.0320, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(151.9543, grad_fn=<MeanBackward0>) tensor(293.0541, grad_fn=<SumBackward0>)\n",
      "tensor(0.0329, grad_fn=<AddBackward0>) tensor(159.9745, grad_fn=<MeanBackward0>) tensor(291.1125, grad_fn=<SumBackward0>)\n",
      "tensor(0.0343, grad_fn=<AddBackward0>) tensor(167.7514, grad_fn=<MeanBackward0>) tensor(289.2082, grad_fn=<SumBackward0>)\n",
      "tensor(0.0340, grad_fn=<AddBackward0>) tensor(163.1478, grad_fn=<MeanBackward0>) tensor(287.3402, grad_fn=<SumBackward0>)\n",
      "tensor(0.0362, grad_fn=<AddBackward0>) tensor(174.2306, grad_fn=<MeanBackward0>) tensor(285.5083, grad_fn=<SumBackward0>)\n",
      "tensor(0.0342, grad_fn=<AddBackward0>) tensor(164.4238, grad_fn=<MeanBackward0>) tensor(283.7115, grad_fn=<SumBackward0>)\n",
      "tensor(0.0312, grad_fn=<AddBackward0>) tensor(154.3642, grad_fn=<MeanBackward0>) tensor(281.9498, grad_fn=<SumBackward0>)\n",
      "Epoch 11/50 — avg train loss: 87.2098\n",
      "tensor(0.0332, grad_fn=<AddBackward0>) tensor(159.7473, grad_fn=<MeanBackward0>) tensor(280.2220, grad_fn=<SumBackward0>)\n",
      "tensor(0.0354, grad_fn=<AddBackward0>) tensor(168.4299, grad_fn=<MeanBackward0>) tensor(278.5280, grad_fn=<SumBackward0>)\n",
      "tensor(0.0334, grad_fn=<AddBackward0>) tensor(161.0549, grad_fn=<MeanBackward0>) tensor(276.8679, grad_fn=<SumBackward0>)\n",
      "tensor(0.0340, grad_fn=<AddBackward0>) tensor(160.4561, grad_fn=<MeanBackward0>) tensor(275.2402, grad_fn=<SumBackward0>)\n",
      "tensor(0.0319, grad_fn=<AddBackward0>) tensor(152.9453, grad_fn=<MeanBackward0>) tensor(273.6439, grad_fn=<SumBackward0>)\n",
      "tensor(0.0340, grad_fn=<AddBackward0>) tensor(160.9520, grad_fn=<MeanBackward0>) tensor(272.0768, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(150.2075, grad_fn=<MeanBackward0>) tensor(270.5392, grad_fn=<SumBackward0>)\n",
      "tensor(0.0337, grad_fn=<AddBackward0>) tensor(161.6667, grad_fn=<MeanBackward0>) tensor(269.0298, grad_fn=<SumBackward0>)\n",
      "Epoch 12/50 — avg train loss: 84.2596\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(151.0521, grad_fn=<MeanBackward0>) tensor(267.5500, grad_fn=<SumBackward0>)\n",
      "tensor(0.0337, grad_fn=<AddBackward0>) tensor(156.3289, grad_fn=<MeanBackward0>) tensor(266.0987, grad_fn=<SumBackward0>)\n",
      "tensor(0.0353, grad_fn=<AddBackward0>) tensor(163.6533, grad_fn=<MeanBackward0>) tensor(264.6752, grad_fn=<SumBackward0>)\n",
      "tensor(0.0335, grad_fn=<AddBackward0>) tensor(157.5120, grad_fn=<MeanBackward0>) tensor(263.2787, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(148.6847, grad_fn=<MeanBackward0>) tensor(261.9086, grad_fn=<SumBackward0>)\n",
      "tensor(0.0371, grad_fn=<AddBackward0>) tensor(167.6267, grad_fn=<MeanBackward0>) tensor(260.5617, grad_fn=<SumBackward0>)\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(148.0001, grad_fn=<MeanBackward0>) tensor(259.2397, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(147.3644, grad_fn=<MeanBackward0>) tensor(257.9408, grad_fn=<SumBackward0>)\n",
      "Epoch 13/50 — avg train loss: 81.7397\n",
      "tensor(0.0343, grad_fn=<AddBackward0>) tensor(155.4806, grad_fn=<MeanBackward0>) tensor(256.6649, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(140.0253, grad_fn=<MeanBackward0>) tensor(255.4113, grad_fn=<SumBackward0>)\n",
      "tensor(0.0373, grad_fn=<AddBackward0>) tensor(169.2962, grad_fn=<MeanBackward0>) tensor(254.1799, grad_fn=<SumBackward0>)\n",
      "tensor(0.0355, grad_fn=<AddBackward0>) tensor(160.5483, grad_fn=<MeanBackward0>) tensor(252.9725, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(147.3753, grad_fn=<MeanBackward0>) tensor(251.7872, grad_fn=<SumBackward0>)\n",
      "tensor(0.0339, grad_fn=<AddBackward0>) tensor(151.4107, grad_fn=<MeanBackward0>) tensor(250.6229, grad_fn=<SumBackward0>)\n",
      "tensor(0.0332, grad_fn=<AddBackward0>) tensor(150.2069, grad_fn=<MeanBackward0>) tensor(249.4775, grad_fn=<SumBackward0>)\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(140.3521, grad_fn=<MeanBackward0>) tensor(248.3501, grad_fn=<SumBackward0>)\n",
      "Epoch 14/50 — avg train loss: 79.8410\n",
      "tensor(0.0343, grad_fn=<AddBackward0>) tensor(154.9864, grad_fn=<MeanBackward0>) tensor(247.2406, grad_fn=<SumBackward0>)\n",
      "tensor(0.0322, grad_fn=<AddBackward0>) tensor(145.2547, grad_fn=<MeanBackward0>) tensor(246.1498, grad_fn=<SumBackward0>)\n",
      "tensor(0.0308, grad_fn=<AddBackward0>) tensor(138.7993, grad_fn=<MeanBackward0>) tensor(245.0774, grad_fn=<SumBackward0>)\n",
      "tensor(0.0344, grad_fn=<AddBackward0>) tensor(149.4160, grad_fn=<MeanBackward0>) tensor(244.0211, grad_fn=<SumBackward0>)\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(139.8033, grad_fn=<MeanBackward0>) tensor(242.9803, grad_fn=<SumBackward0>)\n",
      "tensor(0.0361, grad_fn=<AddBackward0>) tensor(156.7679, grad_fn=<MeanBackward0>) tensor(241.9540, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(139.3313, grad_fn=<MeanBackward0>) tensor(240.9428, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(136.0674, grad_fn=<MeanBackward0>) tensor(239.9447, grad_fn=<SumBackward0>)\n",
      "Epoch 15/50 — avg train loss: 76.3822\n",
      "tensor(0.0323, grad_fn=<AddBackward0>) tensor(140.2506, grad_fn=<MeanBackward0>) tensor(238.9597, grad_fn=<SumBackward0>)\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(134.9500, grad_fn=<MeanBackward0>) tensor(237.9893, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(136.0765, grad_fn=<MeanBackward0>) tensor(237.0318, grad_fn=<SumBackward0>)\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(135.4853, grad_fn=<MeanBackward0>) tensor(236.0854, grad_fn=<SumBackward0>)\n",
      "tensor(0.0338, grad_fn=<AddBackward0>) tensor(142.8838, grad_fn=<MeanBackward0>) tensor(235.1495, grad_fn=<SumBackward0>)\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(134.5788, grad_fn=<MeanBackward0>) tensor(234.2251, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(129.3952, grad_fn=<MeanBackward0>) tensor(233.3128, grad_fn=<SumBackward0>)\n",
      "tensor(0.0318, grad_fn=<AddBackward0>) tensor(133.5219, grad_fn=<MeanBackward0>) tensor(232.4089, grad_fn=<SumBackward0>)\n",
      "Epoch 16/50 — avg train loss: 71.8966\n",
      "tensor(0.0318, grad_fn=<AddBackward0>) tensor(133.5507, grad_fn=<MeanBackward0>) tensor(231.5155, grad_fn=<SumBackward0>)\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(130.6314, grad_fn=<MeanBackward0>) tensor(230.6369, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(134.6022, grad_fn=<MeanBackward0>) tensor(229.7682, grad_fn=<SumBackward0>)\n",
      "tensor(0.0337, grad_fn=<AddBackward0>) tensor(137.6243, grad_fn=<MeanBackward0>) tensor(228.9154, grad_fn=<SumBackward0>)\n",
      "tensor(0.0305, grad_fn=<AddBackward0>) tensor(126.7303, grad_fn=<MeanBackward0>) tensor(228.0981, grad_fn=<SumBackward0>)\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(126.3539, grad_fn=<MeanBackward0>) tensor(227.2890, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(128.7029, grad_fn=<MeanBackward0>) tensor(226.4874, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(129.8576, grad_fn=<MeanBackward0>) tensor(225.7175, grad_fn=<SumBackward0>)\n",
      "Epoch 17/50 — avg train loss: 69.3747\n",
      "tensor(0.0326, grad_fn=<AddBackward0>) tensor(135.9301, grad_fn=<MeanBackward0>) tensor(224.9790, grad_fn=<SumBackward0>)\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(134.7000, grad_fn=<MeanBackward0>) tensor(224.2719, grad_fn=<SumBackward0>)\n",
      "tensor(0.0319, grad_fn=<AddBackward0>) tensor(127.7983, grad_fn=<MeanBackward0>) tensor(223.5725, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(122.8022, grad_fn=<MeanBackward0>) tensor(222.8769, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(121.4837, grad_fn=<MeanBackward0>) tensor(222.1873, grad_fn=<SumBackward0>)\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(124.3068, grad_fn=<MeanBackward0>) tensor(221.5042, grad_fn=<SumBackward0>)\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(122.8361, grad_fn=<MeanBackward0>) tensor(220.8257, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(121.8293, grad_fn=<MeanBackward0>) tensor(220.1525, grad_fn=<SumBackward0>)\n",
      "Epoch 18/50 — avg train loss: 67.0549\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(119.1729, grad_fn=<MeanBackward0>) tensor(219.4845, grad_fn=<SumBackward0>)\n",
      "tensor(0.0322, grad_fn=<AddBackward0>) tensor(125.3861, grad_fn=<MeanBackward0>) tensor(218.8200, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(113.9502, grad_fn=<MeanBackward0>) tensor(218.1594, grad_fn=<SumBackward0>)\n",
      "tensor(0.0368, grad_fn=<AddBackward0>) tensor(134.0575, grad_fn=<MeanBackward0>) tensor(217.5009, grad_fn=<SumBackward0>)\n",
      "tensor(0.0324, grad_fn=<AddBackward0>) tensor(122.2359, grad_fn=<MeanBackward0>) tensor(216.8478, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(113.3307, grad_fn=<MeanBackward0>) tensor(216.1991, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(107.5181, grad_fn=<MeanBackward0>) tensor(215.5555, grad_fn=<SumBackward0>)\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(124.2190, grad_fn=<MeanBackward0>) tensor(214.9278, grad_fn=<SumBackward0>)\n",
      "Epoch 19/50 — avg train loss: 63.8938\n",
      "tensor(0.0337, grad_fn=<AddBackward0>) tensor(145.3675, grad_fn=<MeanBackward0>) tensor(214.3227, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(175.9922, grad_fn=<MeanBackward0>) tensor(213.8218, grad_fn=<SumBackward0>)\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(126.7085, grad_fn=<MeanBackward0>) tensor(213.3335, grad_fn=<SumBackward0>)\n",
      "tensor(0.0383, grad_fn=<AddBackward0>) tensor(142.1615, grad_fn=<MeanBackward0>) tensor(212.8248, grad_fn=<SumBackward0>)\n",
      "tensor(0.0303, grad_fn=<AddBackward0>) tensor(127.1613, grad_fn=<MeanBackward0>) tensor(212.3132, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(128.9571, grad_fn=<MeanBackward0>) tensor(211.7962, grad_fn=<SumBackward0>)\n",
      "tensor(0.0335, grad_fn=<AddBackward0>) tensor(123.4065, grad_fn=<MeanBackward0>) tensor(211.2850, grad_fn=<SumBackward0>)\n",
      "tensor(0.0295, grad_fn=<AddBackward0>) tensor(114.5036, grad_fn=<MeanBackward0>) tensor(210.7705, grad_fn=<SumBackward0>)\n",
      "Epoch 20/50 — avg train loss: 70.6940\n",
      "tensor(0.0325, grad_fn=<AddBackward0>) tensor(121.9160, grad_fn=<MeanBackward0>) tensor(210.2537, grad_fn=<SumBackward0>)\n",
      "tensor(0.0308, grad_fn=<AddBackward0>) tensor(120.3638, grad_fn=<MeanBackward0>) tensor(209.7453, grad_fn=<SumBackward0>)\n",
      "tensor(0.0319, grad_fn=<AddBackward0>) tensor(120.8975, grad_fn=<MeanBackward0>) tensor(209.2437, grad_fn=<SumBackward0>)\n",
      "tensor(0.0293, grad_fn=<AddBackward0>) tensor(110.3979, grad_fn=<MeanBackward0>) tensor(208.7488, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(117.2586, grad_fn=<MeanBackward0>) tensor(208.2605, grad_fn=<SumBackward0>)\n",
      "tensor(0.0318, grad_fn=<AddBackward0>) tensor(119.9680, grad_fn=<MeanBackward0>) tensor(207.7867, grad_fn=<SumBackward0>)\n",
      "tensor(0.0337, grad_fn=<AddBackward0>) tensor(121.7947, grad_fn=<MeanBackward0>) tensor(207.3242, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(113.4736, grad_fn=<MeanBackward0>) tensor(206.8695, grad_fn=<SumBackward0>)\n",
      "Epoch 21/50 — avg train loss: 62.7266\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(117.9882, grad_fn=<MeanBackward0>) tensor(206.4283, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(111.2835, grad_fn=<MeanBackward0>) tensor(206.0005, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(114.1411, grad_fn=<MeanBackward0>) tensor(205.5801, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(108.5180, grad_fn=<MeanBackward0>) tensor(205.1660, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(114.8342, grad_fn=<MeanBackward0>) tensor(204.7606, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(105.3965, grad_fn=<MeanBackward0>) tensor(204.3613, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(108.7706, grad_fn=<MeanBackward0>) tensor(203.9657, grad_fn=<SumBackward0>)\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(106.5458, grad_fn=<MeanBackward0>) tensor(203.5771, grad_fn=<SumBackward0>)\n",
      "Epoch 22/50 — avg train loss: 59.2647\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(103.2844, grad_fn=<MeanBackward0>) tensor(203.1983, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(106.9494, grad_fn=<MeanBackward0>) tensor(202.8242, grad_fn=<SumBackward0>)\n",
      "tensor(0.0301, grad_fn=<AddBackward0>) tensor(106.3947, grad_fn=<MeanBackward0>) tensor(202.4545, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(111.6344, grad_fn=<MeanBackward0>) tensor(202.0919, grad_fn=<SumBackward0>)\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(109.5998, grad_fn=<MeanBackward0>) tensor(201.7410, grad_fn=<SumBackward0>)\n",
      "tensor(0.0283, grad_fn=<AddBackward0>) tensor(101.0323, grad_fn=<MeanBackward0>) tensor(201.3963, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(104.9840, grad_fn=<MeanBackward0>) tensor(201.0549, grad_fn=<SumBackward0>)\n",
      "tensor(0.0280, grad_fn=<AddBackward0>) tensor(99.6695, grad_fn=<MeanBackward0>) tensor(200.7225, grad_fn=<SumBackward0>)\n",
      "Epoch 23/50 — avg train loss: 56.6524\n",
      "tensor(0.0333, grad_fn=<AddBackward0>) tensor(110.2164, grad_fn=<MeanBackward0>) tensor(200.3959, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(101.9173, grad_fn=<MeanBackward0>) tensor(200.0757, grad_fn=<SumBackward0>)\n",
      "tensor(0.0283, grad_fn=<AddBackward0>) tensor(98.4181, grad_fn=<MeanBackward0>) tensor(199.7612, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(100.4003, grad_fn=<MeanBackward0>) tensor(199.4505, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(96.9358, grad_fn=<MeanBackward0>) tensor(199.1426, grad_fn=<SumBackward0>)\n",
      "tensor(0.0323, grad_fn=<AddBackward0>) tensor(105.3300, grad_fn=<MeanBackward0>) tensor(198.8362, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(96.5890, grad_fn=<MeanBackward0>) tensor(198.5344, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(99.4803, grad_fn=<MeanBackward0>) tensor(198.2357, grad_fn=<SumBackward0>)\n",
      "Epoch 24/50 — avg train loss: 54.6035\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(97.4666, grad_fn=<MeanBackward0>) tensor(197.9391, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(96.0591, grad_fn=<MeanBackward0>) tensor(197.6429, grad_fn=<SumBackward0>)\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(95.6806, grad_fn=<MeanBackward0>) tensor(197.3511, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(101.4724, grad_fn=<MeanBackward0>) tensor(197.0619, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(96.9265, grad_fn=<MeanBackward0>) tensor(196.7770, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(92.1029, grad_fn=<MeanBackward0>) tensor(196.4952, grad_fn=<SumBackward0>)\n",
      "tensor(0.0295, grad_fn=<AddBackward0>) tensor(94.9490, grad_fn=<MeanBackward0>) tensor(196.2143, grad_fn=<SumBackward0>)\n",
      "tensor(0.0305, grad_fn=<AddBackward0>) tensor(97.4765, grad_fn=<MeanBackward0>) tensor(195.9385, grad_fn=<SumBackward0>)\n",
      "Epoch 25/50 — avg train loss: 52.4028\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(96.9555, grad_fn=<MeanBackward0>) tensor(195.6719, grad_fn=<SumBackward0>)\n",
      "tensor(0.0312, grad_fn=<AddBackward0>) tensor(110.3935, grad_fn=<MeanBackward0>) tensor(195.4200, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(100.0780, grad_fn=<MeanBackward0>) tensor(195.1823, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(99.2623, grad_fn=<MeanBackward0>) tensor(194.9565, grad_fn=<SumBackward0>)\n",
      "tensor(0.0312, grad_fn=<AddBackward0>) tensor(96.1600, grad_fn=<MeanBackward0>) tensor(194.7343, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(98.9193, grad_fn=<MeanBackward0>) tensor(194.5018, grad_fn=<SumBackward0>)\n",
      "tensor(0.0270, grad_fn=<AddBackward0>) tensor(91.7492, grad_fn=<MeanBackward0>) tensor(194.2597, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(94.5875, grad_fn=<MeanBackward0>) tensor(194.0150, grad_fn=<SumBackward0>)\n",
      "Epoch 26/50 — avg train loss: 53.2087\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(91.6077, grad_fn=<MeanBackward0>) tensor(193.7653, grad_fn=<SumBackward0>)\n",
      "tensor(0.0316, grad_fn=<AddBackward0>) tensor(97.0009, grad_fn=<MeanBackward0>) tensor(193.5115, grad_fn=<SumBackward0>)\n",
      "tensor(0.0293, grad_fn=<AddBackward0>) tensor(90.1291, grad_fn=<MeanBackward0>) tensor(193.2664, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(94.3084, grad_fn=<MeanBackward0>) tensor(193.0211, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(91.7934, grad_fn=<MeanBackward0>) tensor(192.7740, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(89.4438, grad_fn=<MeanBackward0>) tensor(192.5301, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(88.1487, grad_fn=<MeanBackward0>) tensor(192.2893, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(87.4458, grad_fn=<MeanBackward0>) tensor(192.0481, grad_fn=<SumBackward0>)\n",
      "Epoch 27/50 — avg train loss: 49.8408\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(87.5210, grad_fn=<MeanBackward0>) tensor(191.8082, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(85.9171, grad_fn=<MeanBackward0>) tensor(191.5724, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(89.9335, grad_fn=<MeanBackward0>) tensor(191.3326, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(81.9378, grad_fn=<MeanBackward0>) tensor(191.0917, grad_fn=<SumBackward0>)\n",
      "tensor(0.0301, grad_fn=<AddBackward0>) tensor(88.0479, grad_fn=<MeanBackward0>) tensor(190.8570, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(86.3493, grad_fn=<MeanBackward0>) tensor(190.6243, grad_fn=<SumBackward0>)\n",
      "tensor(0.0293, grad_fn=<AddBackward0>) tensor(86.0758, grad_fn=<MeanBackward0>) tensor(190.3963, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(89.3285, grad_fn=<MeanBackward0>) tensor(190.1682, grad_fn=<SumBackward0>)\n",
      "Epoch 28/50 — avg train loss: 47.7953\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(83.3216, grad_fn=<MeanBackward0>) tensor(189.9425, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(88.2337, grad_fn=<MeanBackward0>) tensor(189.7254, grad_fn=<SumBackward0>)\n",
      "tensor(0.0273, grad_fn=<AddBackward0>) tensor(79.9713, grad_fn=<MeanBackward0>) tensor(189.5064, grad_fn=<SumBackward0>)\n",
      "tensor(0.0274, grad_fn=<AddBackward0>) tensor(80.4936, grad_fn=<MeanBackward0>) tensor(189.2840, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(80.1222, grad_fn=<MeanBackward0>) tensor(189.0675, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(84.8289, grad_fn=<MeanBackward0>) tensor(188.8515, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(79.9591, grad_fn=<MeanBackward0>) tensor(188.6325, grad_fn=<SumBackward0>)\n",
      "tensor(0.0293, grad_fn=<AddBackward0>) tensor(86.2749, grad_fn=<MeanBackward0>) tensor(188.4147, grad_fn=<SumBackward0>)\n",
      "Epoch 29/50 — avg train loss: 45.9160\n",
      "tensor(0.0258, grad_fn=<AddBackward0>) tensor(96.8506, grad_fn=<MeanBackward0>) tensor(188.2180, grad_fn=<SumBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>) tensor(120.7389, grad_fn=<MeanBackward0>) tensor(188.0378, grad_fn=<SumBackward0>)\n",
      "tensor(0.0303, grad_fn=<AddBackward0>) tensor(91.7296, grad_fn=<MeanBackward0>) tensor(187.8494, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(92.6169, grad_fn=<MeanBackward0>) tensor(187.6514, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(105.2278, grad_fn=<MeanBackward0>) tensor(187.4568, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(93.0502, grad_fn=<MeanBackward0>) tensor(187.2695, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(99.6749, grad_fn=<MeanBackward0>) tensor(187.0712, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(98.6372, grad_fn=<MeanBackward0>) tensor(186.8621, grad_fn=<SumBackward0>)\n",
      "Epoch 30/50 — avg train loss: 53.4670\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(92.0304, grad_fn=<MeanBackward0>) tensor(186.6485, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(89.1322, grad_fn=<MeanBackward0>) tensor(186.4281, grad_fn=<SumBackward0>)\n",
      "tensor(0.0308, grad_fn=<AddBackward0>) tensor(86.1271, grad_fn=<MeanBackward0>) tensor(186.2022, grad_fn=<SumBackward0>)\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(80.0402, grad_fn=<MeanBackward0>) tensor(185.9742, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(82.0256, grad_fn=<MeanBackward0>) tensor(185.7458, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(83.8175, grad_fn=<MeanBackward0>) tensor(185.5185, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(83.8285, grad_fn=<MeanBackward0>) tensor(185.2943, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(84.5697, grad_fn=<MeanBackward0>) tensor(185.0735, grad_fn=<SumBackward0>)\n",
      "Epoch 31/50 — avg train loss: 46.8014\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(84.4000, grad_fn=<MeanBackward0>) tensor(184.8568, grad_fn=<SumBackward0>)\n",
      "tensor(0.0318, grad_fn=<AddBackward0>) tensor(83.2081, grad_fn=<MeanBackward0>) tensor(184.6453, grad_fn=<SumBackward0>)\n",
      "tensor(0.0255, grad_fn=<AddBackward0>) tensor(75.0063, grad_fn=<MeanBackward0>) tensor(184.4378, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(77.0983, grad_fn=<MeanBackward0>) tensor(184.2336, grad_fn=<SumBackward0>)\n",
      "tensor(0.0265, grad_fn=<AddBackward0>) tensor(75.4236, grad_fn=<MeanBackward0>) tensor(184.0338, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(76.6426, grad_fn=<MeanBackward0>) tensor(183.8379, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(76.9401, grad_fn=<MeanBackward0>) tensor(183.6456, grad_fn=<SumBackward0>)\n",
      "tensor(0.0303, grad_fn=<AddBackward0>) tensor(79.0493, grad_fn=<MeanBackward0>) tensor(183.4568, grad_fn=<SumBackward0>)\n",
      "Epoch 32/50 — avg train loss: 43.6926\n",
      "tensor(0.0290, grad_fn=<AddBackward0>) tensor(78.4214, grad_fn=<MeanBackward0>) tensor(183.2733, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(82.8710, grad_fn=<MeanBackward0>) tensor(183.0968, grad_fn=<SumBackward0>)\n",
      "tensor(0.0274, grad_fn=<AddBackward0>) tensor(72.9638, grad_fn=<MeanBackward0>) tensor(182.9237, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(76.2614, grad_fn=<MeanBackward0>) tensor(182.7533, grad_fn=<SumBackward0>)\n",
      "tensor(0.0266, grad_fn=<AddBackward0>) tensor(71.7110, grad_fn=<MeanBackward0>) tensor(182.5876, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(73.7790, grad_fn=<MeanBackward0>) tensor(182.4248, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(75.0059, grad_fn=<MeanBackward0>) tensor(182.2631, grad_fn=<SumBackward0>)\n",
      "tensor(0.0264, grad_fn=<AddBackward0>) tensor(70.5287, grad_fn=<MeanBackward0>) tensor(182.1042, grad_fn=<SumBackward0>)\n",
      "Epoch 33/50 — avg train loss: 42.1489\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(73.5969, grad_fn=<MeanBackward0>) tensor(181.9460, grad_fn=<SumBackward0>)\n",
      "tensor(0.0266, grad_fn=<AddBackward0>) tensor(70.3000, grad_fn=<MeanBackward0>) tensor(181.7897, grad_fn=<SumBackward0>)\n",
      "tensor(0.0311, grad_fn=<AddBackward0>) tensor(75.6441, grad_fn=<MeanBackward0>) tensor(181.6330, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(71.2736, grad_fn=<MeanBackward0>) tensor(181.4845, grad_fn=<SumBackward0>)\n",
      "tensor(0.0248, grad_fn=<AddBackward0>) tensor(70.8624, grad_fn=<MeanBackward0>) tensor(181.3372, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(73.4569, grad_fn=<MeanBackward0>) tensor(181.2115, grad_fn=<SumBackward0>)\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(77.8742, grad_fn=<MeanBackward0>) tensor(181.0873, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(70.6001, grad_fn=<MeanBackward0>) tensor(180.9618, grad_fn=<SumBackward0>)\n",
      "Epoch 34/50 — avg train loss: 41.0820\n",
      "tensor(0.0266, grad_fn=<AddBackward0>) tensor(68.8221, grad_fn=<MeanBackward0>) tensor(180.8376, grad_fn=<SumBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>) tensor(71.8227, grad_fn=<MeanBackward0>) tensor(180.7093, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(69.6174, grad_fn=<MeanBackward0>) tensor(180.5854, grad_fn=<SumBackward0>)\n",
      "tensor(0.0256, grad_fn=<AddBackward0>) tensor(65.2587, grad_fn=<MeanBackward0>) tensor(180.4591, grad_fn=<SumBackward0>)\n",
      "tensor(0.0273, grad_fn=<AddBackward0>) tensor(67.3767, grad_fn=<MeanBackward0>) tensor(180.3325, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(72.4713, grad_fn=<MeanBackward0>) tensor(180.2043, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(77.5396, grad_fn=<MeanBackward0>) tensor(180.0791, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(77.1423, grad_fn=<MeanBackward0>) tensor(179.9622, grad_fn=<SumBackward0>)\n",
      "Epoch 35/50 — avg train loss: 40.2713\n",
      "tensor(0.0265, grad_fn=<AddBackward0>) tensor(71.6325, grad_fn=<MeanBackward0>) tensor(179.8576, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(69.2272, grad_fn=<MeanBackward0>) tensor(179.7410, grad_fn=<SumBackward0>)\n",
      "tensor(0.0276, grad_fn=<AddBackward0>) tensor(69.3945, grad_fn=<MeanBackward0>) tensor(179.6217, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(66.0585, grad_fn=<MeanBackward0>) tensor(179.5043, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(72.1216, grad_fn=<MeanBackward0>) tensor(179.3840, grad_fn=<SumBackward0>)\n",
      "tensor(0.0255, grad_fn=<AddBackward0>) tensor(64.3485, grad_fn=<MeanBackward0>) tensor(179.2640, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(64.4055, grad_fn=<MeanBackward0>) tensor(179.1407, grad_fn=<SumBackward0>)\n",
      "tensor(0.0268, grad_fn=<AddBackward0>) tensor(68.2253, grad_fn=<MeanBackward0>) tensor(179.0145, grad_fn=<SumBackward0>)\n",
      "Epoch 36/50 — avg train loss: 38.8395\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(64.9691, grad_fn=<MeanBackward0>) tensor(178.8909, grad_fn=<SumBackward0>)\n",
      "tensor(0.0243, grad_fn=<AddBackward0>) tensor(62.5872, grad_fn=<MeanBackward0>) tensor(178.7655, grad_fn=<SumBackward0>)\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(68.2452, grad_fn=<MeanBackward0>) tensor(178.6396, grad_fn=<SumBackward0>)\n",
      "tensor(0.0277, grad_fn=<AddBackward0>) tensor(64.2370, grad_fn=<MeanBackward0>) tensor(178.5137, grad_fn=<SumBackward0>)\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(66.5042, grad_fn=<MeanBackward0>) tensor(178.3864, grad_fn=<SumBackward0>)\n",
      "tensor(0.0274, grad_fn=<AddBackward0>) tensor(65.0702, grad_fn=<MeanBackward0>) tensor(178.2583, grad_fn=<SumBackward0>)\n",
      "tensor(0.0268, grad_fn=<AddBackward0>) tensor(62.6731, grad_fn=<MeanBackward0>) tensor(178.1291, grad_fn=<SumBackward0>)\n",
      "tensor(0.0251, grad_fn=<AddBackward0>) tensor(64.3571, grad_fn=<MeanBackward0>) tensor(178.0066, grad_fn=<SumBackward0>)\n",
      "Epoch 37/50 — avg train loss: 37.2865\n",
      "tensor(0.0251, grad_fn=<AddBackward0>) tensor(69.3481, grad_fn=<MeanBackward0>) tensor(177.9030, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(64.4454, grad_fn=<MeanBackward0>) tensor(177.8242, grad_fn=<SumBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>) tensor(68.0522, grad_fn=<MeanBackward0>) tensor(177.7379, grad_fn=<SumBackward0>)\n",
      "tensor(0.0254, grad_fn=<AddBackward0>) tensor(62.1837, grad_fn=<MeanBackward0>) tensor(177.6477, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(69.1212, grad_fn=<MeanBackward0>) tensor(177.5619, grad_fn=<SumBackward0>)\n",
      "tensor(0.0274, grad_fn=<AddBackward0>) tensor(77.1964, grad_fn=<MeanBackward0>) tensor(177.4833, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(76.6618, grad_fn=<MeanBackward0>) tensor(177.4117, grad_fn=<SumBackward0>)\n",
      "tensor(0.0280, grad_fn=<AddBackward0>) tensor(65.0510, grad_fn=<MeanBackward0>) tensor(177.3330, grad_fn=<SumBackward0>)\n",
      "Epoch 38/50 — avg train loss: 39.1318\n",
      "tensor(0.0301, grad_fn=<AddBackward0>) tensor(73.5164, grad_fn=<MeanBackward0>) tensor(177.2393, grad_fn=<SumBackward0>)\n",
      "tensor(0.0258, grad_fn=<AddBackward0>) tensor(70.3974, grad_fn=<MeanBackward0>) tensor(177.1319, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(67.2750, grad_fn=<MeanBackward0>) tensor(177.0103, grad_fn=<SumBackward0>)\n",
      "tensor(0.0270, grad_fn=<AddBackward0>) tensor(62.1964, grad_fn=<MeanBackward0>) tensor(176.8831, grad_fn=<SumBackward0>)\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(62.9796, grad_fn=<MeanBackward0>) tensor(176.7525, grad_fn=<SumBackward0>)\n",
      "tensor(0.0276, grad_fn=<AddBackward0>) tensor(60.4461, grad_fn=<MeanBackward0>) tensor(176.6205, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(61.9933, grad_fn=<MeanBackward0>) tensor(176.4862, grad_fn=<SumBackward0>)\n",
      "tensor(0.0260, grad_fn=<AddBackward0>) tensor(60.0423, grad_fn=<MeanBackward0>) tensor(176.3498, grad_fn=<SumBackward0>)\n",
      "Epoch 39/50 — avg train loss: 37.2246\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(62.1961, grad_fn=<MeanBackward0>) tensor(176.2123, grad_fn=<SumBackward0>)\n",
      "tensor(0.0270, grad_fn=<AddBackward0>) tensor(60.3332, grad_fn=<MeanBackward0>) tensor(176.0768, grad_fn=<SumBackward0>)\n",
      "tensor(0.0259, grad_fn=<AddBackward0>) tensor(60.1119, grad_fn=<MeanBackward0>) tensor(175.9433, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(67.0993, grad_fn=<MeanBackward0>) tensor(175.8276, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(64.0987, grad_fn=<MeanBackward0>) tensor(175.7237, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(59.0043, grad_fn=<MeanBackward0>) tensor(175.6236, grad_fn=<SumBackward0>)\n",
      "tensor(0.0265, grad_fn=<AddBackward0>) tensor(57.4936, grad_fn=<MeanBackward0>) tensor(175.5201, grad_fn=<SumBackward0>)\n",
      "tensor(0.0262, grad_fn=<AddBackward0>) tensor(58.3385, grad_fn=<MeanBackward0>) tensor(175.4141, grad_fn=<SumBackward0>)\n",
      "Epoch 40/50 — avg train loss: 35.4788\n",
      "tensor(0.0259, grad_fn=<AddBackward0>) tensor(56.5293, grad_fn=<MeanBackward0>) tensor(175.3068, grad_fn=<SumBackward0>)\n",
      "tensor(0.0266, grad_fn=<AddBackward0>) tensor(56.2258, grad_fn=<MeanBackward0>) tensor(175.1987, grad_fn=<SumBackward0>)\n",
      "tensor(0.0277, grad_fn=<AddBackward0>) tensor(56.6406, grad_fn=<MeanBackward0>) tensor(175.0899, grad_fn=<SumBackward0>)\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(58.7029, grad_fn=<MeanBackward0>) tensor(174.9805, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(57.8803, grad_fn=<MeanBackward0>) tensor(174.8712, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(110.6461, grad_fn=<MeanBackward0>) tensor(174.7816, grad_fn=<SumBackward0>)\n",
      "tensor(0.0311, grad_fn=<AddBackward0>) tensor(121.3563, grad_fn=<MeanBackward0>) tensor(174.6956, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(79.3835, grad_fn=<MeanBackward0>) tensor(174.5773, grad_fn=<SumBackward0>)\n",
      "Epoch 41/50 — avg train loss: 41.5638\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(62.7855, grad_fn=<MeanBackward0>) tensor(174.4251, grad_fn=<SumBackward0>)\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(77.6013, grad_fn=<MeanBackward0>) tensor(174.2470, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(68.3412, grad_fn=<MeanBackward0>) tensor(174.0485, grad_fn=<SumBackward0>)\n",
      "tensor(0.0308, grad_fn=<AddBackward0>) tensor(65.9408, grad_fn=<MeanBackward0>) tensor(173.8397, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(68.7813, grad_fn=<MeanBackward0>) tensor(173.6309, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(61.3354, grad_fn=<MeanBackward0>) tensor(173.4242, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(60.9650, grad_fn=<MeanBackward0>) tensor(173.2195, grad_fn=<SumBackward0>)\n",
      "tensor(0.0273, grad_fn=<AddBackward0>) tensor(60.4060, grad_fn=<MeanBackward0>) tensor(173.0160, grad_fn=<SumBackward0>)\n",
      "Epoch 42/50 — avg train loss: 37.4991\n",
      "tensor(0.0267, grad_fn=<AddBackward0>) tensor(59.1117, grad_fn=<MeanBackward0>) tensor(172.8156, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(58.5935, grad_fn=<MeanBackward0>) tensor(172.6196, grad_fn=<SumBackward0>)\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(58.8420, grad_fn=<MeanBackward0>) tensor(172.4311, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(57.4955, grad_fn=<MeanBackward0>) tensor(172.2503, grad_fn=<SumBackward0>)\n",
      "tensor(0.0293, grad_fn=<AddBackward0>) tensor(59.5732, grad_fn=<MeanBackward0>) tensor(172.0758, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(57.7622, grad_fn=<MeanBackward0>) tensor(171.9139, grad_fn=<SumBackward0>)\n",
      "tensor(0.0317, grad_fn=<AddBackward0>) tensor(59.9604, grad_fn=<MeanBackward0>) tensor(171.7594, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(57.3292, grad_fn=<MeanBackward0>) tensor(171.6125, grad_fn=<SumBackward0>)\n",
      "Epoch 43/50 — avg train loss: 34.1899\n",
      "tensor(0.0268, grad_fn=<AddBackward0>) tensor(54.0369, grad_fn=<MeanBackward0>) tensor(171.4714, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(55.0805, grad_fn=<MeanBackward0>) tensor(171.3377, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(53.7098, grad_fn=<MeanBackward0>) tensor(171.2074, grad_fn=<SumBackward0>)\n",
      "tensor(0.0277, grad_fn=<AddBackward0>) tensor(53.8747, grad_fn=<MeanBackward0>) tensor(171.0862, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(54.8779, grad_fn=<MeanBackward0>) tensor(170.9686, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(55.5504, grad_fn=<MeanBackward0>) tensor(170.8559, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(56.4989, grad_fn=<MeanBackward0>) tensor(170.7466, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(51.1195, grad_fn=<MeanBackward0>) tensor(170.6459, grad_fn=<SumBackward0>)\n",
      "Epoch 44/50 — avg train loss: 32.2265\n",
      "tensor(0.0240, grad_fn=<AddBackward0>) tensor(49.0309, grad_fn=<MeanBackward0>) tensor(170.5468, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(53.3589, grad_fn=<MeanBackward0>) tensor(170.4483, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(52.5540, grad_fn=<MeanBackward0>) tensor(170.3517, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(50.2341, grad_fn=<MeanBackward0>) tensor(170.2606, grad_fn=<SumBackward0>)\n",
      "tensor(0.0268, grad_fn=<AddBackward0>) tensor(50.8809, grad_fn=<MeanBackward0>) tensor(170.1716, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(53.3553, grad_fn=<MeanBackward0>) tensor(170.0848, grad_fn=<SumBackward0>)\n",
      "tensor(0.0280, grad_fn=<AddBackward0>) tensor(58.4689, grad_fn=<MeanBackward0>) tensor(170.0061, grad_fn=<SumBackward0>)\n",
      "tensor(0.0239, grad_fn=<AddBackward0>) tensor(48.6333, grad_fn=<MeanBackward0>) tensor(169.9323, grad_fn=<SumBackward0>)\n",
      "Epoch 45/50 — avg train loss: 31.1622\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(54.0025, grad_fn=<MeanBackward0>) tensor(169.8583, grad_fn=<SumBackward0>)\n",
      "tensor(0.0265, grad_fn=<AddBackward0>) tensor(51.2499, grad_fn=<MeanBackward0>) tensor(169.7863, grad_fn=<SumBackward0>)\n",
      "tensor(0.0268, grad_fn=<AddBackward0>) tensor(50.5962, grad_fn=<MeanBackward0>) tensor(169.7149, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(48.9011, grad_fn=<MeanBackward0>) tensor(169.6434, grad_fn=<SumBackward0>)\n",
      "tensor(0.0264, grad_fn=<AddBackward0>) tensor(47.7550, grad_fn=<MeanBackward0>) tensor(169.5703, grad_fn=<SumBackward0>)\n",
      "tensor(0.0280, grad_fn=<AddBackward0>) tensor(49.6321, grad_fn=<MeanBackward0>) tensor(169.4954, grad_fn=<SumBackward0>)\n",
      "tensor(0.0247, grad_fn=<AddBackward0>) tensor(47.8273, grad_fn=<MeanBackward0>) tensor(169.4183, grad_fn=<SumBackward0>)\n",
      "tensor(0.0251, grad_fn=<AddBackward0>) tensor(48.9850, grad_fn=<MeanBackward0>) tensor(169.3390, grad_fn=<SumBackward0>)\n",
      "Epoch 46/50 — avg train loss: 30.1441\n",
      "tensor(0.0258, grad_fn=<AddBackward0>) tensor(45.5789, grad_fn=<MeanBackward0>) tensor(169.2630, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(48.2945, grad_fn=<MeanBackward0>) tensor(169.1850, grad_fn=<SumBackward0>)\n",
      "tensor(0.0266, grad_fn=<AddBackward0>) tensor(45.4441, grad_fn=<MeanBackward0>) tensor(169.1087, grad_fn=<SumBackward0>)\n",
      "tensor(0.0283, grad_fn=<AddBackward0>) tensor(47.9521, grad_fn=<MeanBackward0>) tensor(169.0313, grad_fn=<SumBackward0>)\n",
      "tensor(0.0245, grad_fn=<AddBackward0>) tensor(42.9097, grad_fn=<MeanBackward0>) tensor(168.9535, grad_fn=<SumBackward0>)\n",
      "tensor(0.0266, grad_fn=<AddBackward0>) tensor(45.5323, grad_fn=<MeanBackward0>) tensor(168.8728, grad_fn=<SumBackward0>)\n",
      "tensor(0.0264, grad_fn=<AddBackward0>) tensor(54.3259, grad_fn=<MeanBackward0>) tensor(168.7982, grad_fn=<SumBackward0>)\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(86.8438, grad_fn=<MeanBackward0>) tensor(168.7395, grad_fn=<SumBackward0>)\n",
      "Epoch 47/50 — avg train loss: 31.1268\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(48.6045, grad_fn=<MeanBackward0>) tensor(168.6742, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(63.6660, grad_fn=<MeanBackward0>) tensor(168.5871, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(56.6746, grad_fn=<MeanBackward0>) tensor(168.4892, grad_fn=<SumBackward0>)\n",
      "tensor(0.0262, grad_fn=<AddBackward0>) tensor(62.1605, grad_fn=<MeanBackward0>) tensor(168.3900, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(49.6634, grad_fn=<MeanBackward0>) tensor(168.2852, grad_fn=<SumBackward0>)\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(54.8866, grad_fn=<MeanBackward0>) tensor(168.1699, grad_fn=<SumBackward0>)\n",
      "tensor(0.0251, grad_fn=<AddBackward0>) tensor(48.8318, grad_fn=<MeanBackward0>) tensor(168.0479, grad_fn=<SumBackward0>)\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(51.5343, grad_fn=<MeanBackward0>) tensor(167.9213, grad_fn=<SumBackward0>)\n",
      "Epoch 48/50 — avg train loss: 32.1762\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(55.7669, grad_fn=<MeanBackward0>) tensor(167.7935, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(52.6882, grad_fn=<MeanBackward0>) tensor(167.6665, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(57.1700, grad_fn=<MeanBackward0>) tensor(167.5417, grad_fn=<SumBackward0>)\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(60.2038, grad_fn=<MeanBackward0>) tensor(167.4338, grad_fn=<SumBackward0>)\n",
      "tensor(0.0265, grad_fn=<AddBackward0>) tensor(57.5962, grad_fn=<MeanBackward0>) tensor(167.3281, grad_fn=<SumBackward0>)\n",
      "tensor(0.0267, grad_fn=<AddBackward0>) tensor(46.5765, grad_fn=<MeanBackward0>) tensor(167.2219, grad_fn=<SumBackward0>)\n",
      "tensor(0.0243, grad_fn=<AddBackward0>) tensor(55.1838, grad_fn=<MeanBackward0>) tensor(167.1193, grad_fn=<SumBackward0>)\n",
      "tensor(0.0270, grad_fn=<AddBackward0>) tensor(51.2534, grad_fn=<MeanBackward0>) tensor(167.0264, grad_fn=<SumBackward0>)\n",
      "Epoch 49/50 — avg train loss: 32.1577\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(52.5446, grad_fn=<MeanBackward0>) tensor(166.9317, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(51.1499, grad_fn=<MeanBackward0>) tensor(166.8360, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(53.1171, grad_fn=<MeanBackward0>) tensor(166.7454, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(50.4828, grad_fn=<MeanBackward0>) tensor(166.6608, grad_fn=<SumBackward0>)\n",
      "tensor(0.0265, grad_fn=<AddBackward0>) tensor(46.4868, grad_fn=<MeanBackward0>) tensor(166.5780, grad_fn=<SumBackward0>)\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(47.6833, grad_fn=<MeanBackward0>) tensor(166.4953, grad_fn=<SumBackward0>)\n",
      "tensor(0.0267, grad_fn=<AddBackward0>) tensor(49.6927, grad_fn=<MeanBackward0>) tensor(166.4126, grad_fn=<SumBackward0>)\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(49.2878, grad_fn=<MeanBackward0>) tensor(166.3306, grad_fn=<SumBackward0>)\n",
      "Epoch 50/50 — avg train loss: 30.0951\n",
      "Detected anomalies in test set: 35 / 115\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # DAGMM Anomaly Detection with Sampled Anomalies in Train\n",
    "\n",
    "# %% [code]\n",
    "# 1) Make your DAGMM code importable\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"cnn_dagmm\"))\n",
    "\n",
    "# %% [code]\n",
    "# 2) Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from collections import Counter\n",
    "\n",
    "from model import DAGMM  # your revised model.py\n",
    "\n",
    "# %% [code]\n",
    "# 3) Data transforms & loaders (images → flattened vectors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# Print ground‑truth counts\n",
    "print(\"Train class counts:\", {train_ds.classes[k]: v for k,v in Counter(train_ds.targets).items()})\n",
    "print(\"Test  class counts:\", {test_ds.classes[k]: v for k,v in Counter(test_ds.targets).items()})\n",
    "\n",
    "# %% [code]\n",
    "# 4) Model + optimizer\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_features = 3 * 64 * 64\n",
    "\n",
    "model = DAGMM(\n",
    "    input_dim        = 3 * 64 * 64,     # still required by signature but not forwarded to CompressionNetwork\n",
    "    latent_dim       = 90,\n",
    "    n_gmm_components = 5,\n",
    "    comp_kwargs      = {'latent_dim': 90},  # now cleanly matches CompressionNetwork\n",
    "    est_kwargs       = {'hidden_dims': [128], 'activation': torch.nn.Tanh, 'dropout': 0.3},\n",
    "    device           = device\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# %% [code]\n",
    "# 5) Training loop\n",
    "n_epochs = 50\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        x = imgs.to(device) \n",
    "        out  = model(x)\n",
    "        loss = model.loss_function(x, out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}/{n_epochs} — avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# %% [code]\n",
    "# 6) Scoring test set & thresholding\n",
    "model.eval()\n",
    "energies = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in test_loader:\n",
    "        x = imgs.to(device)\n",
    "        energies.append(model(x)['energy'].cpu())\n",
    "energies = torch.cat(energies)\n",
    "\n",
    "# 95th‐percentile threshold\n",
    "thr = energies.quantile(0.70)\n",
    "mask = energies >xs thr\n",
    "print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# %% [code]\n",
    "# 7) (Optional) Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "plt.axvline(thr, color='r', linestyle='--', label='66% threshold')\n",
    "plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64e7a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for top 30% anomalies: 75.58619689941406\n",
      "\n",
      "Confusion matrix:\n",
      "[[14  6]\n",
      " [ 5 90]]\n",
      "\n",
      "Accuracy: 90.43%\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anomaly       0.74      0.70      0.72        20\n",
      "      normal       0.94      0.95      0.94        95\n",
      "\n",
      "    accuracy                           0.90       115\n",
      "   macro avg       0.84      0.82      0.83       115\n",
      "weighted avg       0.90      0.90      0.90       115\n",
      "\n",
      "\n",
      "ROC‑AUC (energy as score): 0.873\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# 1) Recompute energies and collect true labels\n",
    "model.eval()\n",
    "energies = []\n",
    "y_true   = []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        x = imgs.to(device)                       # CNN takes [B,3,64,64]\n",
    "        out = model(x)\n",
    "        energies.append(out['energy'].cpu())      # [B]\n",
    "        y_true.append(labels)\n",
    "energies = torch.cat(energies)                  # [N_test]\n",
    "y_true   = torch.cat(y_true)                    # [N_test]\n",
    "\n",
    "# 2) Identify top 30% highest‐energy samples as anomalies\n",
    "#    70th percentile cutoff → top 30% above this\n",
    "thr = energies.quantile(0.84)\n",
    "\n",
    "\n",
    "# 3) Predictions\n",
    "y_pred = (energies < thr).int()\n",
    "\n",
    "# 4) Metrics\n",
    "print(\"Threshold for top 30% anomalies:\", thr.item())\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "acc = (y_true == y_pred).float().mean() * 100\n",
    "print(f\"\\nAccuracy: {acc:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_ds.classes))\n",
    "\n",
    "# 5) ROC‑AUC (still informative even though we fix the cutoff by proportion)\n",
    "auc = roc_auc_score(y_true, -energies)  # invert since lower energy = more normal\n",
    "print(f\"\\nROC‑AUC (energy as score): {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c220e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [code]\n",
    "# # Hyperparameter grid search for DAGMM\n",
    "# import itertools\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms, datasets\n",
    "# from model import DAGMM\n",
    "\n",
    "# # 1) Data loaders (reuse from above)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((64,64)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "# test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 2) Define search space\n",
    "# param_grid = {\n",
    "#     \"latent_dim\":       [2, 5, 10],\n",
    "#     \"n_gmm_components\": [2, 4, 6],\n",
    "#     \"est_hidden\":       [[32], [64, 32]],\n",
    "#     \"dropout\":          [0.1, 0.3],\n",
    "#     \"lr\":               [1e-3, 1e-4],\n",
    "# }\n",
    "\n",
    "# # 3) Helper to train & evaluate one config\n",
    "# def run_experiment(cfg):\n",
    "#     # build model\n",
    "#     model = DAGMM(\n",
    "#         input_dim        = 3*64*64,\n",
    "#         latent_dim       = cfg[\"latent_dim\"],\n",
    "#         n_gmm_components = cfg[\"n_gmm_components\"],\n",
    "#         comp_kwargs      = {\"latent_dim\": cfg[\"latent_dim\"]},\n",
    "#         est_kwargs       = {\n",
    "#             \"input_dim\": cfg[\"latent_dim\"]+2,\n",
    "#             \"output_dim\": cfg[\"n_gmm_components\"],\n",
    "#             \"hidden_dims\": cfg[\"est_hidden\"],\n",
    "#             \"activation\": torch.nn.ReLU,\n",
    "#             \"dropout\": cfg[\"dropout\"]\n",
    "#         },\n",
    "#         device=device\n",
    "#     ).to(device)\n",
    "#     opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
    "\n",
    "#     # train for fixed epochs\n",
    "#     for epoch in range(1, 11):\n",
    "#         model.train()\n",
    "#         for imgs, _ in train_loader:\n",
    "#             x = imgs.to(device)\n",
    "#             out = model(x)\n",
    "#             loss = model.loss_function(x, out)\n",
    "#             opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "#     # evaluate: compute energy scores & threshold at 90th percentile\n",
    "#     model.eval()\n",
    "#     energies, y_true = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for imgs, labels in test_loader:\n",
    "#             x = imgs.to(device)\n",
    "#             energies.append(model(x)[\"energy\"].cpu())\n",
    "#             y_true.append(labels)\n",
    "#     energies = torch.cat(energies)\n",
    "#     y_true   = torch.cat(y_true)\n",
    "\n",
    "#     thr = energies.quantile(0.9)\n",
    "#     y_pred = (energies > thr).int()\n",
    "#     acc = (y_pred == y_true).float().mean().item()\n",
    "\n",
    "#     return acc\n",
    "\n",
    "# # 4) Run grid search\n",
    "# results = []\n",
    "# for vals in itertools.product(*param_grid.values()):\n",
    "#     cfg = dict(zip(param_grid.keys(), vals))\n",
    "#     acc = run_experiment(cfg)\n",
    "#     print(f\"Config {cfg} → accuracy {acc:.3f}\")\n",
    "#     results.append({**cfg, \"accuracy\": acc})\n",
    "\n",
    "# # 5) Summarize\n",
    "# df = pd.DataFrame(results)\n",
    "# print(\"\\nTop 5 configs by accuracy:\")\n",
    "# print(df.sort_values(\"accuracy\", ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09632df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(\"accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88341822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import matplotlib.pyplot as plt\n",
    "# from model import DAGMM\n",
    "\n",
    "# # 1) Synthesize 100 images\n",
    "# white_imgs   = torch.ones(80, 3, 64, 64)\n",
    "# colored_imgs = torch.zeros(20, 3, 64, 64); colored_imgs[:,0] = 1.0\n",
    "# labels       = torch.cat([torch.zeros(80), torch.ones(20)]).long()\n",
    "\n",
    "# # single dataset\n",
    "# dataset = TensorDataset(torch.cat([white_imgs, colored_imgs], 0), labels)\n",
    "\n",
    "# # train loader (shuffle)\n",
    "# train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# # eval loader (no shuffle)\n",
    "# eval_loader  = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# # 2) Build & train DAGMM\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = DAGMM(\n",
    "#     input_dim=3*64*64, latent_dim=2, n_gmm_components=2,\n",
    "#     comp_kwargs={'latent_dim':2},\n",
    "#     est_kwargs ={\n",
    "#         'input_dim': 2+2,\n",
    "#         'output_dim': 2,\n",
    "#         'hidden_dims':[4],\n",
    "#         'activation':torch.nn.ReLU,\n",
    "#         'dropout':0.1\n",
    "#     },\n",
    "#     device=device\n",
    "# ).to(device)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # 3) Train on white only\n",
    "# for epoch in range(1, 51):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     n_whites = 0\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         mask = (y_batch == 0)\n",
    "#         if not mask.any():\n",
    "#             continue\n",
    "#         x = x_batch[mask].to(device)\n",
    "#         out = model(x)\n",
    "#         loss = model.loss_function(x, out)\n",
    "#         opt.zero_grad()\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "\n",
    "#         total_loss += loss.item() * x.size(0)\n",
    "#         n_whites   += x.size(0)\n",
    "\n",
    "#     if epoch % 10 == 0 and n_whites > 0:\n",
    "#         print(f\"Epoch {epoch:02d} — avg white-loss: {total_loss / n_whites:.4f}\")\n",
    "\n",
    "# # 4) Compute energies & collect labels\n",
    "# model.eval()\n",
    "# energies = []\n",
    "# labels_list = []\n",
    "# with torch.no_grad():\n",
    "#     for x_batch, y_batch in eval_loader:\n",
    "#         x = x_batch.to(device)\n",
    "#         out = model(x)\n",
    "#         energies.append(out['energy'].cpu())\n",
    "#         labels_list.append(y_batch)\n",
    "\n",
    "# energies = torch.cat(energies)     # [100]\n",
    "# labels   = torch.cat(labels_list)  # [100]\n",
    "\n",
    "# # 5) Inspect distributions by label\n",
    "# white_e = energies[labels == 0]\n",
    "# red_e   = energies[labels == 1]\n",
    "\n",
    "# print(\"White  μ±σ:\", white_e.mean().item(), white_e.std().item())\n",
    "# print(\"Red    μ±σ:\",   red_e.mean().item(),   red_e.std().item())\n",
    "\n",
    "# plt.hist(white_e, bins=20, alpha=0.6, label='white')\n",
    "# plt.hist(red_e,   bins=20, alpha=0.6, label='red')\n",
    "# plt.legend(); plt.show()\n",
    "\n",
    "# # 6) Mid-point threshold\n",
    "# thr = (white_e.mean() + red_e.mean()) / 2\n",
    "# pred = (energies > thr).int()\n",
    "\n",
    "# print(\"Detected anomalies:\", pred[labels==1].sum().item(), \"/ 20\")\n",
    "# print(\"False positives:   \", pred[labels==0].sum().item(), \"/ 80\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "959f0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_curve, f1_score, precision_recall_fscore_support\n",
    "\n",
    "# # assume `energies` [100] and `labels` [100] (0=white,1=red)\n",
    "# y_true = labels.numpy()\n",
    "# y_score = energies.numpy()\n",
    "\n",
    "# # 1) ROC curve and optimal threshold at max (TPR–FPR)\n",
    "# fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "# opt_idx = np.argmax(tpr - fpr)\n",
    "# opt_thr = thresholds[opt_idx]\n",
    "# print(f\"Optimal threshold from ROC: {opt_thr:.4f} (TPR={tpr[opt_idx]:.2f}, FPR={fpr[opt_idx]:.2f})\")\n",
    "\n",
    "# # 2) Build predictions & compute metrics\n",
    "# y_pred = (energies > opt_thr).int().numpy()\n",
    "# tp, fp, fn, tn = (\n",
    "#     ((y_true==1)&(y_pred==1)).sum(),\n",
    "#     ((y_true==0)&(y_pred==1)).sum(),\n",
    "#     ((y_true==1)&(y_pred==0)).sum(),\n",
    "#     ((y_true==0)&(y_pred==0)).sum(),\n",
    "# )\n",
    "# print(\"Confusion matrix:\")\n",
    "# print(f\"  [[TP={tp}, FP={fp}]\\n   [FN={fn}, TN={tn}]]\")\n",
    "\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "# print(f\"\\nPrecision (white,red): {precision}\")\n",
    "# print(f\"Recall    (white,red): {recall}\")\n",
    "# print(f\" F1‑score (white,red): {f1}\")\n",
    "\n",
    "# # 3) (Optional) print overall F1\n",
    "# print(f\"\\nOverall F1: {f1_score(y_true, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f3dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6200.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6969.60it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 1293.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "IMG_SIZE = 64\n",
    "NORMAL_SHAPES = ['circle', 'square']\n",
    "NORMAL_COLORS = ['red', 'green', 'blue']\n",
    "NUM_NORMAL = 200\n",
    "NUM_ANOMALY = 20\n",
    "OUTPUT_DIR = \"dataset\"\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Utility: Create folders\n",
    "def make_dirs():\n",
    "    for split in ['train', 'test']:\n",
    "        for cls in ['normal', 'anomalous']:\n",
    "            os.makedirs(os.path.join(OUTPUT_DIR, split, cls), exist_ok=True)\n",
    "\n",
    "# Generate a normal image\n",
    "def generate_normal_image():\n",
    "    img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), 'black')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    shape = random.choice(NORMAL_SHAPES)\n",
    "    color = random.choice(NORMAL_COLORS)\n",
    "    x0, y0, x1, y1 = 16, 16, 48, 48\n",
    "\n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([x0, y0, x1, y1], fill=color)\n",
    "    elif shape == 'square':\n",
    "        draw.rectangle([x0, y0, x1, y1], fill=color)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Generate an anomalous image\n",
    "def generate_anomalous_image():\n",
    "    img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), 'black')\n",
    "    choice = random.choice(['noise', 'triangle'])\n",
    "\n",
    "    if choice == 'noise':\n",
    "        pixels = img.load()\n",
    "        for i in range(IMG_SIZE):\n",
    "            for j in range(IMG_SIZE):\n",
    "                val = random.randint(0, 255)\n",
    "                pixels[i, j] = (val, val, val)\n",
    "    else:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.polygon([(32, 10), (50, 50), (14, 50)], fill='white')\n",
    "    return img\n",
    "\n",
    "# Save image\n",
    "def save_image(img, split, cls, idx):\n",
    "    path = os.path.join(OUTPUT_DIR, split, cls, f\"{cls}_{idx:03d}.png\")\n",
    "    img.save(path)\n",
    "\n",
    "# Main generation logic\n",
    "def generate_dataset():\n",
    "    make_dirs()\n",
    "\n",
    "    # TRAIN: 100 normal\n",
    "    print(\"Generating training set...\")\n",
    "    for i in tqdm(range(100)):\n",
    "        img = generate_normal_image()\n",
    "        save_image(img, 'train', 'normal', i)\n",
    "\n",
    "    # TEST: 100 normal + 20 anomalous\n",
    "    print(\"Generating test set...\")\n",
    "    for i in tqdm(range(100)):\n",
    "        img = generate_normal_image()\n",
    "        save_image(img, 'test', 'normal', i)\n",
    "    for i in tqdm(range(20)):\n",
    "        img = generate_anomalous_image()\n",
    "        save_image(img, 'test', 'anomalous', i)\n",
    "\n",
    "generate_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
